{"cells":[{"cell_type":"code","metadata":{"source_hash":"d882e67b","execution_start":1746060006323,"execution_millis":3061,"execution_context_id":"9bc9b4cf-88d5-4447-b389-b23f2c2f64db","cell_id":"e2fb978589d246539b594e60420d4590","deepnote_cell_type":"code"},"source":"!pip install optuna==4.3.0","block_group":"dfea632d815c4f5bb5582d06f42c3da9","execution_count":4,"outputs":[{"name":"stdout","text":"Collecting optuna==4.3.0\n  Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sqlalchemy>=1.4.2 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from optuna==4.3.0) (1.4.42)\nRequirement already satisfied: packaging>=20.0 in /root/venv/lib/python3.10/site-packages (from optuna==4.3.0) (24.2)\nCollecting colorlog\n  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: alembic>=1.5.0 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from optuna==4.3.0) (1.13.2)\nRequirement already satisfied: numpy in /root/venv/lib/python3.10/site-packages (from optuna==4.3.0) (1.25.2)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.10/site-packages (from optuna==4.3.0) (6.0.2)\nRequirement already satisfied: tqdm in /root/venv/lib/python3.10/site-packages (from optuna==4.3.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=4 in /root/venv/lib/python3.10/site-packages (from alembic>=1.5.0->optuna==4.3.0) (4.12.2)\nRequirement already satisfied: Mako in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from alembic>=1.5.0->optuna==4.3.0) (1.3.5)\nRequirement already satisfied: greenlet!=0.4.17 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna==4.3.0) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /root/venv/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna==4.3.0) (3.0.2)\nInstalling collected packages: colorlog, optuna\nSuccessfully installed colorlog-6.9.0 optuna-4.3.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6094f85a-fc89-4573-a901-c3c34056bbe5","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4122838c","execution_start":1746060049810,"execution_millis":2778,"execution_context_id":"9bc9b4cf-88d5-4447-b389-b23f2c2f64db","cell_id":"d06a702ee9354166b6fef4a1e88faf89","deepnote_cell_type":"code"},"source":"!pip install optuna-integration[sklearn]","block_group":"d965a50596d14258ada3535d9f14775f","execution_count":11,"outputs":[{"name":"stdout","text":"Collecting optuna-integration[sklearn]\n  Downloading optuna_integration-4.3.0-py3-none-any.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: optuna in /root/venv/lib/python3.10/site-packages (from optuna-integration[sklearn]) (4.3.0)\nRequirement already satisfied: scikit-learn in /root/venv/lib/python3.10/site-packages (from optuna-integration[sklearn]) (1.6.1)\nRequirement already satisfied: scipy in /root/venv/lib/python3.10/site-packages (from optuna-integration[sklearn]) (1.15.2)\nRequirement already satisfied: pandas in /root/venv/lib/python3.10/site-packages (from optuna-integration[sklearn]) (2.1.4)\nRequirement already satisfied: tqdm in /root/venv/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (4.67.1)\nRequirement already satisfied: numpy in /root/venv/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (1.25.2)\nRequirement already satisfied: colorlog in /root/venv/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (6.9.0)\nRequirement already satisfied: packaging>=20.0 in /root/venv/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (24.2)\nRequirement already satisfied: alembic>=1.5.0 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (1.13.2)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (6.0.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from optuna->optuna-integration[sklearn]) (1.4.42)\nRequirement already satisfied: tzdata>=2022.1 in /root/venv/lib/python3.10/site-packages (from pandas->optuna-integration[sklearn]) (2025.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /root/venv/lib/python3.10/site-packages (from pandas->optuna-integration[sklearn]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /root/venv/lib/python3.10/site-packages (from pandas->optuna-integration[sklearn]) (2025.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /root/venv/lib/python3.10/site-packages (from scikit-learn->optuna-integration[sklearn]) (3.5.0)\nRequirement already satisfied: joblib>=1.2.0 in /root/venv/lib/python3.10/site-packages (from scikit-learn->optuna-integration[sklearn]) (1.4.2)\nRequirement already satisfied: Mako in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->optuna-integration[sklearn]) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /root/venv/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->optuna-integration[sklearn]) (4.12.2)\nRequirement already satisfied: six>=1.5 in /root/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->optuna-integration[sklearn]) (1.17.0)\nRequirement already satisfied: greenlet!=0.4.17 in /toolkit-cache/0.2.14/python3.10/kernel-libs/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[sklearn]) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /root/venv/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[sklearn]) (3.0.2)\nInstalling collected packages: optuna-integration\nSuccessfully installed optuna-integration-4.3.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/af0c4235-2777-4df8-8858-b424464fe893","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"57bcafb688d84c9190f4598add00b03f","deepnote_cell_type":"code"},"source":"","block_group":"d15008d7ffa34b61a6db63dab7f0ab99","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"eac195f7","execution_start":1746060062976,"execution_millis":7303,"execution_context_id":"9bc9b4cf-88d5-4447-b389-b23f2c2f64db","cell_id":"fe834998b529415f80db5528f3aea797","deepnote_cell_type":"code"},"source":"!pip install shap==0.47.2","block_group":"45dfb035f167422a86411832025aa930","execution_count":17,"outputs":[{"name":"stdout","text":"Collecting shap==0.47.2\n  Downloading shap-0.47.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (992 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.3/992.3 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cloudpickle in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (2.2.1)\nRequirement already satisfied: typing-extensions in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (4.12.2)\nRequirement already satisfied: scikit-learn in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (1.6.1)\nRequirement already satisfied: packaging>20.9 in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (24.2)\nCollecting numba>=0.54\n  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting slicer==0.0.8\n  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\nRequirement already satisfied: tqdm>=4.27.0 in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (4.67.1)\nRequirement already satisfied: numpy in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (1.25.2)\nRequirement already satisfied: scipy in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (1.15.2)\nRequirement already satisfied: pandas in /root/venv/lib/python3.10/site-packages (from shap==0.47.2) (2.1.4)\nCollecting llvmlite<0.45,>=0.44.0dev0\n  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /root/venv/lib/python3.10/site-packages (from pandas->shap==0.47.2) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.1 in /root/venv/lib/python3.10/site-packages (from pandas->shap==0.47.2) (2025.1)\nRequirement already satisfied: pytz>=2020.1 in /root/venv/lib/python3.10/site-packages (from pandas->shap==0.47.2) (2025.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /root/venv/lib/python3.10/site-packages (from scikit-learn->shap==0.47.2) (3.5.0)\nRequirement already satisfied: joblib>=1.2.0 in /root/venv/lib/python3.10/site-packages (from scikit-learn->shap==0.47.2) (1.4.2)\nRequirement already satisfied: six>=1.5 in /root/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap==0.47.2) (1.17.0)\nInstalling collected packages: slicer, llvmlite, numba, shap\nSuccessfully installed llvmlite-0.44.0 numba-0.61.2 shap-0.47.2 slicer-0.0.8\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6743e072-ca91-4624-915a-811950125d74","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"8f367dbc","execution_start":1746060196220,"execution_millis":1,"execution_context_id":"9bc9b4cf-88d5-4447-b389-b23f2c2f64db","cell_id":"fc02bf6dd12943a6b250f35ff5bbc55f","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nimport os\n\ndef load_and_preprocess_data(data_path):\n    \"\"\"Загружает и выполняет базовую предобработку данных SKEMPI с отладочными сообщениями.\"\"\"\n    print(f\"\\n--- [DEBUG] Загрузка данных из: {data_path} ---\")\n    if not os.path.exists(data_path):\n        raise FileNotFoundError(f\"Файл {data_path} не найден.\")\n\n    # Определяем имена колонок ЗАРАНЕЕ\n    expected_column_names = ['#Pdb', 'Mutation(s)_PDB', 'Mutation(s)_cleaned', 'iMutation_Location(s)', 'Hold_out_type', 'Hold_out_proteins', 'Affinity_mut (M)', 'Affinity_mut_parsed', 'Affinity_wt (M)', 'Affinity_wt_parsed', 'Reference', 'Protein 1', 'Protein 2', 'Temperature', 'kon_mut (M^(-1)s^(-1))', 'kon_mut_parsed', 'kon_wt (M^(-1)s^(-1))', 'kon_wt_parsed', 'koff_mut (s^(-1))', 'koff_mut_parsed', 'koff_wt (s^(-1))', 'koff_wt_parsed', 'dH_mut (kcal mol^(-1))', 'dH_wt (kcal mol^(-1))', 'dS_mut (cal mol^(-1) K^(-1))', 'dS_wt (cal mol^(-1) K^(-1))', 'Notes', 'Method', 'SKEMPI version']\n\n    # Пытаемся прочитать с заголовком из первой строки\n    try:\n        print(\"[DEBUG] Попытка чтения с header=0, sep=';'...\")\n        df = pd.read_csv(data_path, sep=';', header=0, low_memory=False) # Added low_memory=False\n        print(f\"[DEBUG] Размер после загрузки (header=0): {df.shape}\")\n        # Если колонки не прочитались как надо, присваиваем стандартные\n        if len(df.columns) != len(expected_column_names):\n             print(f\"[DEBUG] ВНИМАНИЕ: Кол-во колонок ({len(df.columns)}) не совпадает с ожидаемым ({len(expected_column_names)}). Переименовываем...\")\n             if len(df.columns) < len(expected_column_names):\n                 df.columns = expected_column_names[:len(df.columns)]\n             else: # Больше колонок, чем ожидалось\n                 df.columns = expected_column_names + [f'extra_{i}' for i in range(len(df.columns) - len(expected_column_names))]\n        else:\n             # На всякий случай переименуем, если первая строка была не совсем заголовком\n             df.columns = expected_column_names\n             print(\"[DEBUG] Колонки переименованы в стандартные.\")\n\n    except Exception as e:\n        print(f\"[DEBUG] Ошибка при чтении CSV: {e}\")\n        # Попробуем без header если первая попытка не удалась или колонки не совпали\n        try:\n            print(\"[DEBUG] Попытка чтения без header, с присвоением имен...\")\n            df = pd.read_csv(data_path, sep=';', header=None, names=expected_column_names, low_memory=False)\n            print(f\"[DEBUG] Размер после загрузки (header=None): {df.shape}\")\n            # Удалим первую строку, если она была заголовком\n            if df.iloc[0]['#Pdb'] == '#Pdb': # Проверяем, похожа ли первая строка на заголовок\n                df = df.iloc[1:].reset_index(drop=True)\n                print(\"[DEBUG] Первая строка, похожая на заголовок, удалена.\")\n        except Exception as e2:\n             print(f\"[DEBUG] Вторая ошибка при чтении CSV: {e2}\")\n             return pd.DataFrame(), [], []\n\n\n    if df.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст сразу после загрузки.\")\n        return pd.DataFrame(), [], []\n\n    # --- Фильтрация Мутаций (ОСТОРОЖНО) ---\n    if 'Mutation(s)_cleaned' not in df.columns:\n        print(\"[DEBUG] ОШИБКА: Столбец 'Mutation(s)_cleaned' не найден после загрузки/переименования.\")\n        return pd.DataFrame(), [], []\n\n    original_rows = df.shape[0]\n    print(f\"[DEBUG] Начальный размер перед фильтрацией мутаций: {original_rows}\")\n\n    # 1. Убираем NaN в колонке мутаций\n    df = df.dropna(subset=['Mutation(s)_cleaned'])\n    print(f\"[DEBUG] Размер после dropna('Mutation(s)_cleaned'): {df.shape} (удалено {original_rows - df.shape[0]})\")\n    if df.empty: return pd.DataFrame(), [], []\n\n    # 2. Извлекаем WT и MUT аминокислоты (первый и последний символ)\n    # Преобразуем в строку на всякий случай\n    df['WT_AA'] = df['Mutation(s)_cleaned'].astype(str).str[0]\n    df['MUT_AA'] = df['Mutation(s)_cleaned'].astype(str).str[-1]\n    print(\"[DEBUG] WT_AA и MUT_AA извлечены.\")\n\n    # 3. Проверяем, что WT и MUT - валидные аминокислоты (буквы из словаря)\n    aa_properties = { 'A': {'volume': 88.6, 'hydrophobicity': 1.8}, 'R': {'volume': 173.4, 'hydrophobicity': -4.5}, 'N': {'volume': 114.1, 'hydrophobicity': -3.5}, 'D': {'volume': 111.1, 'hydrophobicity': -3.5}, 'C': {'volume': 108.5, 'hydrophobicity': 2.5}, 'Q': {'volume': 143.8, 'hydrophobicity': -3.5}, 'E': {'volume': 138.4, 'hydrophobicity': -3.5}, 'G': {'volume': 60.1, 'hydrophobicity': -0.4}, 'H': {'volume': 153.2, 'hydrophobicity': -3.2}, 'I': {'volume': 166.7, 'hydrophobicity': 4.5}, 'L': {'volume': 166.7, 'hydrophobicity': 3.8}, 'K': {'volume': 168.6, 'hydrophobicity': -3.9}, 'M': {'volume': 162.9, 'hydrophobicity': 1.9}, 'F': {'volume': 189.9, 'hydrophobicity': 2.8}, 'P': {'volume': 112.7, 'hydrophobicity': -1.6}, 'S': {'volume': 89.0, 'hydrophobicity': -0.8}, 'T': {'volume': 116.1, 'hydrophobicity': -0.7}, 'W': {'volume': 227.8, 'hydrophobicity': -0.9}, 'Y': {'volume': 193.6, 'hydrophobicity': -1.3}, 'V': {'volume': 140.0, 'hydrophobicity': 4.2}}\n    valid_aa = set(aa_properties.keys())\n    original_rows_aa_filter = df.shape[0]\n    df = df[df['WT_AA'].isin(valid_aa) & df['MUT_AA'].isin(valid_aa)]\n    print(f\"[DEBUG] Размер после фильтрации невалидных WT/MUT AA: {df.shape} (удалено {original_rows_aa_filter - df.shape[0]})\")\n    if df.empty:\n         print(\"[DEBUG] ОШИБКА: DataFrame пуст после фильтрации невалидных WT/MUT AA.\")\n         return pd.DataFrame(), [], []\n\n    # 4. Убираем мутации A->A, C->C и т.д.\n    original_rows_self_mut = df.shape[0]\n    df = df[df['WT_AA'] != df['MUT_AA']]\n    print(f\"[DEBUG] Размер после удаления мутаций A->A: {df.shape} (удалено {original_rows_self_mut - df.shape[0]})\")\n    if df.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст после удаления мутаций A->A.\")\n        return pd.DataFrame(), [], []\n\n    # --- Остальная обработка (как в v2) ---\n    # Обработка температуры\n    if 'Temperature' in df.columns:\n        df['Temperature'] = df['Temperature'].astype(str).str.replace(r'\\(assumed\\)', '', regex=True)\n        # Handle ranges like '298-310' by taking the average or first value\n        def parse_temp(temp_str):\n            if '-' in temp_str:\n                try:\n                    low, high = map(float, temp_str.split('-'))\n                    return (low + high) / 2\n                except:\n                    return np.nan\n            else:\n                return pd.to_numeric(temp_str, errors='coerce')\n\n        df['Temperature'] = df['Temperature'].apply(parse_temp)\n        nan_before = df['Temperature'].isna().sum()\n        if df['Temperature'].notna().any() and nan_before < len(df): # Check if there are non-NaNs to impute from\n            try:\n                 imputer = KNNImputer(n_neighbors=5)\n                 df['Temperature'] = imputer.fit_transform(df[['Temperature']])\n            except ValueError as e:\n                 print(f\"[DEBUG] Warning: KNNImputer failed for Temperature ({e}). Filling NaN with mean.\")\n                 mean_temp = df['Temperature'].mean()\n                 df['Temperature'] = df['Temperature'].fillna(mean_temp if not pd.isna(mean_temp) else 298.0)\n        else: # All NaNs or no NaNs\n            mean_temp = df['Temperature'].mean()\n            df['Temperature'] = df['Temperature'].fillna(mean_temp if not pd.isna(mean_temp) else 298.0) # Fill remaining NaNs (if any) or if all were NaNs\n\n        print(f\"[DEBUG] Температура обработана. NaN до (после парсинга): {nan_before}, NaN после: {df['Temperature'].isna().sum()}\")\n    else:\n        print(\"[DEBUG] ВНИМАНИЕ: Столбец 'Temperature' не найден. Используем T=298K.\")\n        df['Temperature'] = 298.0\n\n    # Вычисление ddG\n    if 'Affinity_mut_parsed' in df.columns and 'Affinity_wt_parsed' in df.columns:\n        R = 1.987e-3\n        df['Kd_mut'] = pd.to_numeric(df['Affinity_mut_parsed'], errors='coerce')\n        df['Kd_wt'] = pd.to_numeric(df['Affinity_wt_parsed'], errors='coerce')\n        mask = (df['Kd_mut'].notnull()) & (df['Kd_wt'].notnull()) & \\\n               (df['Kd_wt'] > 0) & (df['Kd_mut'] > 0) & \\\n               (df['Temperature'].notnull()) & (df['Temperature'] > 0)\n        df['ddG'] = np.nan\n        df.loc[mask, 'ddG'] = R * df.loc[mask, 'Temperature'] * np.log(df.loc[mask, 'Kd_mut'] / df.loc[mask, 'Kd_wt']) # Note: original formula had -R, SKEMPI ddG = RT*ln(Kd_mut/Kd_wt)\n        print(f\"[DEBUG] Строк с валидными Kd/T для ddG: {mask.sum()}\")\n        original_rows = df.shape[0]\n        df = df.dropna(subset=['ddG'])\n        print(f\"[DEBUG] Размер после dropna(ddG): {df.shape} (удалено {original_rows - df.shape[0]})\")\n        if df.empty: return pd.DataFrame(), [], []\n        original_rows = df.shape[0]\n        df = df[df['ddG'] != 0]\n        print(f\"[DEBUG] Размер после удаления ddG==0: {df.shape} (удалено {original_rows - df.shape[0]})\")\n        if df.empty: return pd.DataFrame(), [], []\n        # Sign definition: positive ddG -> destabilizing (weaker binding Kd_mut > Kd_wt) -> Class 0\n        #                  negative ddG -> stabilizing (stronger binding Kd_mut < Kd_wt) -> Class 1\n        df['sign_ddG'] = (df['ddG'] < 0).astype(int)\n        print(f\"[DEBUG] Распределение sign_ddG (1=стабилизация, 0=дестабилизация):\\n{df['sign_ddG'].value_counts(normalize=True)}\")\n\n    else:\n         print(\"[DEBUG] ОШИБКА: Столбцы аффинности не найдены.\")\n         return pd.DataFrame(), [], []\n\n    # Инженерия признаков\n    df['Position'] = df['Mutation(s)_cleaned'].astype(str).str.extract(r'(\\d+)', expand=False).astype(float)\n    if 'iMutation_Location(s)' in df.columns:\n        df['Location'] = df['iMutation_Location(s)'].fillna('Unknown')\n        # Map detailed locations to broader categories if desired (like in previous script)\n        location_map = {'COR': 'COR', 'RIM': 'SUR', 'SUR': 'SUR', 'SUP': 'INT', 'INT': 'INT'}\n        df['Location_General'] = df['Location'].map(location_map).fillna('Other') # Use a general category\n        print(f\"[DEBUG] Location_General categories:\\n{df['Location_General'].value_counts()}\")\n    else:\n        print(\"[DEBUG] ВНИМАНИЕ: столбец 'iMutation_Location(s)' не найден.\")\n        df['Location'] = 'Unknown'\n        df['Location_General'] = 'Unknown'\n\n\n    # Физико-химические свойства\n    for prop in ['volume', 'hydrophobicity']:\n        prop_map = {aa: props[prop] for aa, props in aa_properties.items()}\n        wt_props = df['WT_AA'].map(prop_map)\n        mut_props = df['MUT_AA'].map(prop_map)\n        df[f'delta_{prop}'] = mut_props - wt_props\n        df[f'WT_{prop}'] = wt_props\n        df[f'MUT_{prop}'] = mut_props\n    print(\"[DEBUG] Физ.-хим. признаки рассчитаны.\")\n\n    # Use Location_General for features\n    categorical_features = ['WT_AA', 'MUT_AA', 'Location_General']\n    numeric_features = ['Position', 'delta_volume', 'delta_hydrophobicity',\n                       'WT_volume', 'WT_hydrophobicity', 'MUT_volume', 'MUT_hydrophobicity']\n    final_feature_cols = categorical_features + numeric_features\n    target_col = 'sign_ddG'\n\n    missing_features = [col for col in final_feature_cols if col not in df.columns]\n    if missing_features:\n        print(f\"[DEBUG] ОШИБКА: Отсутствуют колонки признаков: {missing_features}\")\n        return pd.DataFrame(), [], []\n    if target_col not in df.columns:\n         print(f\"[DEBUG] ОШИБКА: Отсутствует целевая колонка '{target_col}'.\")\n         return pd.DataFrame(), [], []\n\n    # Drop NaNs from final feature columns + target BEFORE returning\n    original_rows = df.shape[0]\n    df_clean = df[final_feature_cols + [target_col]].copy() # Select only needed cols before dropna\n    df_clean = df_clean.dropna(subset=final_feature_cols + [target_col])\n    print(f\"[DEBUG] Размер после dropna(признаки+цель): {df_clean.shape} (удалено {original_rows - df_clean.shape[0]})\")\n\n    # Ensure numeric types after potential NaNs\n    for col in numeric_features:\n        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n    original_rows = df_clean.shape[0]\n    df_clean = df_clean.dropna(subset=numeric_features) # Drop again if coerce created NaNs\n    print(f\"[DEBUG] Размер после dropna(num_features post-convert): {df_clean.shape} (удалено {original_rows - df_clean.shape[0]})\")\n\n    if df_clean.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст перед возвратом.\")\n    else:\n        print(f\"--- [DEBUG] Данные успешно обработаны. Финальный размер: {df_clean.shape} ---\")\n\n    # Return the cleaned dataframe and the correct feature lists\n    return df_clean, categorical_features, numeric_features","block_group":"1e8a764d5617493192c760d6a6c19b87","execution_count":23,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9739cfa9","execution_start":1746060198520,"execution_millis":420,"execution_context_id":"9bc9b4cf-88d5-4447-b389-b23f2c2f64db","cell_id":"fabff6e5c3d144c2906d8c9d292654f5","deepnote_cell_type":"code"},"source":"# ========= 1. Импорты и Настройки =========\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nfrom joblib import Memory\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (accuracy_score, balanced_accuracy_score, cohen_kappa_score,\n                             f1_score, matthews_corrcoef, log_loss, roc_auc_score,\n                             average_precision_score, make_scorer, confusion_matrix)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import Pipeline as SklearnPipeline\nfrom sklearn.base import clone\nfrom sklearn.impute import KNNImputer\nfrom scipy.sparse import issparse\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom optuna.integration import OptunaSearchCV\nfrom optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution\nimport shap\nimport json\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Конфигурация\nDATA_PATH = 'skempi_v2.csv'\nLOCATION_TO_ANALYZE = 'Other'\nTEST_SIZE = 0.25\nRANDOM_STATE = 42\nCV_FOLDS = 5\nN_FEATURES_TO_SELECT = 30\n\n# ========= 2. Расширенная Предобработка =========\ndef enhanced_preprocessing(df):\n    \"\"\"Добавление структурных и эволюционных признаков\"\"\"\n    # Добавление структурных признаков (гипотетических значений)\n    df['delta_rsa'] = np.random.normal(0, 1, len(df))  # ΔSASA\n    df['conservation_score'] = np.random.normal(0.5, 0.2, len(df))  # Консервативность\n    \n    # Расширенные физико-химические свойства\n    aa_properties = {\n        'A': {'volume': 88.6, 'hydrophobicity': 1.8, 'polarity': 0},\n        'R': {'volume': 173.4, 'hydrophobicity': -4.5, 'polarity': 1},\n        # ...остальные аминокислоты с дополнительными свойствами\n    }\n    \n    # Добавление новых признаков\n    df['charge_diff'] = df['MUT_AA'].map(lambda x: 1 if x in 'KR' else (-1 if x in 'DE' else 0)) - \\\n                        df['WT_AA'].map(lambda x: 1 if x in 'KR' else (-1 if x in 'DE' else 0))\n    \n    return df\n\n# ========= 3. Загрузка и Подготовка Данных =========\ndf_processed, categorical_features, numeric_features = load_and_preprocess_data(DATA_PATH)\ndf_processed = enhanced_preprocessing(df_processed)\n\n# Фильтрация по локации\nif LOCATION_TO_ANALYZE == 'Other':\n    df_loc = df_processed[df_processed['Location_General'] != 'COR']\nelif LOCATION_TO_ANALYZE == 'COR':\n    df_loc = df_processed[df_processed['Location_General'] == 'COR']\nelse:\n    df_loc = df_processed.copy()\n\n# Добавление новых признаков в список\nnumeric_features.extend(['delta_rsa', 'conservation_score', 'charge_diff'])\n\nX = df_loc[categorical_features + numeric_features]\ny = df_loc['sign_ddG']\n\n# Разделение данных\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n)\n\n# ========= 4. Расширенный Препроцессор =========\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ],\n    remainder='passthrough',\n    verbose_feature_names_out=False\n)\n\n# ========= 5. Лучшие Модели с Гиперпараметрической Оптимизацией =========\n# Базовые модели для ансамбля\nrf_params = {\n    'n_estimators': IntDistribution(100, 500),\n    'max_depth': IntDistribution(3, 20),\n    'min_samples_split': IntDistribution(2, 11),\n    'class_weight': CategoricalDistribution(['balanced', None])\n}\n\ngb_params = {\n    'n_estimators': IntDistribution(100, 500),\n    'learning_rate': FloatDistribution(0.01, 0.3),\n    'max_depth': IntDistribution(3, 15),\n    'subsample': FloatDistribution(0.6, 1.0)\n}\n\nsvc_params = {\n    'C': FloatDistribution(0.1, 100),\n    'kernel': CategoricalDistribution(['rbf', 'poly']),\n    'gamma': CategoricalDistribution(['scale', 'auto'])\n}\n\n# Создание оптимизаторов\nopt_rf = OptunaSearchCV(\n    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n    param_distributions=rf_params,\n    cv=StratifiedKFold(n_splits=CV_FOLDS),\n    n_trials=30,\n    scoring='f1',\n    n_jobs=-1\n)\n\nopt_gb = OptunaSearchCV(\n    estimator=GradientBoostingClassifier(random_state=RANDOM_STATE),\n    param_distributions=gb_params,\n    cv=StratifiedKFold(n_splits=CV_FOLDS),\n    n_trials=30,\n    scoring='f1',\n    n_jobs=-1\n)\n\n# ========= 6. Ансамблевая Модель =========\nstacking_model = StackingClassifier(\n    estimators=[\n        ('rf', opt_rf),\n        ('gb', opt_gb),\n        ('lr', LogisticRegression(max_iter=1000))\n    ],\n    final_estimator=LogisticRegression(),\n    cv=StratifiedKFold(n_splits=CV_FOLDS)\n)\n\n# ========= 7. Пайплайн с Ресэмплингом =========\npipeline = ImbPipeline([\n    ('preprocessor', preprocessor),\n    ('feature_selector', SelectKBest(score_func=f_classif, k=N_FEATURES_TO_SELECT)),\n    ('resampler', SMOTE(random_state=RANDOM_STATE)),\n    ('classifier', stacking_model)\n])\n\n# ========= 8. Обучение и Оценка =========\n# Обучение\npipeline.fit(X_train, y_train)\n\n# Предсказание\ny_pred = pipeline.predict(X_test)\ny_proba = pipeline.predict_proba(X_test)[:, 1]\n\n# Расчет метрик\nmetrics = calculate_metrics(y_test, y_pred, y_proba)\n\n# ========= 9. Интерпретация =========\n# Получение важных признаков\nexplainer = shap.Explainer(pipeline.named_steps['classifier'])\nshap_values = explainer.shap_values(pipeline.named_steps['preprocessor'].transform(X_test))\n\n# Визуализация\nplt.figure(figsize=(12, 6))\nshap.summary_plot(shap_values, pipeline.named_steps['preprocessor'].transform(X_test))\nplt.tight_layout()\nplt.savefig('shap_summary.png')\n\n# ========= 10. Сохранение Результатов =========\nresults = {\n    'best_params': {\n        'rf': opt_rf.best_params_,\n        'gb': opt_gb.best_params_\n    },\n    'metrics': metrics,\n    'feature_importance': dict(zip(\n        pipeline.named_steps['preprocessor'].get_feature_names_out(),\n        pipeline.named_steps['classifier'].feature_importances_\n    ))\n}\n\nwith open('enhanced_model_results.json', 'w') as f:\n    json.dump(results, f, indent=4)\n\n# Сохранение модели\nimport joblib\njoblib.dump(pipeline, 'enhanced_model.joblib')\n\nprint(\"Расширенная модель успешно обучена и сохранена!\")","block_group":"fabff6e5c3d144c2906d8c9d292654f5","execution_count":null,"outputs":[{"name":"stdout","text":"\n--- [DEBUG] Загрузка данных из: skempi_v2.csv ---\n[DEBUG] Попытка чтения с header=0, sep=';'...\n[DEBUG] Размер после загрузки (header=0): (7085, 29)\n[DEBUG] Колонки переименованы в стандартные.\n[DEBUG] Начальный размер перед фильтрацией мутаций: 7085\n[DEBUG] Размер после dropna('Mutation(s)_cleaned'): (7085, 29) (удалено 0)\n[DEBUG] WT_AA и MUT_AA извлечены.\n[DEBUG] Размер после фильтрации невалидных WT/MUT AA: (7085, 31) (удалено 0)\n[DEBUG] Размер после удаления мутаций A->A: (6992, 31) (удалено 93)\n[I 2025-05-01 00:43:18,850] A new study created in memory with name: no-name-cb2983c6-b947-47df-b783-b00fcc8c59ed\n[DEBUG] Температура обработана. NaN до (после парсинга): 4, NaN после: 0\n[DEBUG] Строк с валидными Kd/T для ddG: 6719\n[DEBUG] Размер после dropna(ddG): (6719, 34) (удалено 273)\n[DEBUG] Размер после удаления ddG==0: (6531, 34) (удалено 188)\n[DEBUG] Распределение sign_ddG (1=стабилизация, 0=дестабилизация):\nsign_ddG\n0    0.78181\n1    0.21819\nName: proportion, dtype: float64\n[DEBUG] Location_General categories:\nLocation_General\nCOR      2123\nOther    1752\nSUR      1639\nINT      1017\nName: count, dtype: int64\n[DEBUG] Физ.-хим. признаки рассчитаны.\n[DEBUG] Размер после dropna(признаки+цель): (6531, 11) (удалено 0)\n[DEBUG] Размер после dropna(num_features post-convert): (6531, 11) (удалено 0)\n--- [DEBUG] Данные успешно обработаны. Финальный размер: (6531, 11) ---\n[I 2025-05-01 00:43:24,555] Trial 0 finished with value: 0.7894935124155524 and parameters: {'n_estimators': 111, 'max_depth': 18, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7894935124155524.\n[I 2025-05-01 00:43:25,353] Trial 1 finished with value: 0.745926412482713 and parameters: {'n_estimators': 160, 'max_depth': 9, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7894935124155524.\n[I 2025-05-01 00:43:27,569] Trial 4 finished with value: 0.6426978565198338 and parameters: {'n_estimators': 103, 'max_depth': 3, 'min_samples_split': 2, 'class_weight': None}. Best is trial 0 with value: 0.7894935124155524.\n[I 2025-05-01 00:43:36,849] Trial 6 finished with value: 0.6470594281412316 and parameters: {'n_estimators': 312, 'max_depth': 3, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7894935124155524.\n[I 2025-05-01 00:43:37,206] Trial 5 finished with value: 0.7869420973700303 and parameters: {'n_estimators': 224, 'max_depth': 20, 'min_samples_split': 2, 'class_weight': None}. Best is trial 0 with value: 0.7894935124155524.\n[I 2025-05-01 00:43:38,643] Trial 3 finished with value: 0.7923483272030433 and parameters: {'n_estimators': 400, 'max_depth': 18, 'min_samples_split': 10, 'class_weight': None}. Best is trial 3 with value: 0.7923483272030433.\n[I 2025-05-01 00:43:39,023] Trial 2 finished with value: 0.7731485445838862 and parameters: {'n_estimators': 454, 'max_depth': 11, 'min_samples_split': 4, 'class_weight': None}. Best is trial 3 with value: 0.7923483272030433.\n[I 2025-05-01 00:43:53,854] Trial 9 finished with value: 0.7963922495775568 and parameters: {'n_estimators': 315, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:43:53,951] Trial 8 finished with value: 0.6961889995591886 and parameters: {'n_estimators': 500, 'max_depth': 5, 'min_samples_split': 11, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:43:54,005] Trial 7 finished with value: 0.7871741524603808 and parameters: {'n_estimators': 370, 'max_depth': 13, 'min_samples_split': 6, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:43:59,513] Trial 12 finished with value: 0.7912625463913626 and parameters: {'n_estimators': 113, 'max_depth': 20, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:02,410] Trial 11 finished with value: 0.7917765838034635 and parameters: {'n_estimators': 171, 'max_depth': 18, 'min_samples_split': 5, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:02,959] Trial 10 finished with value: 0.7905434366785007 and parameters: {'n_estimators': 496, 'max_depth': 19, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:06,290] Trial 13 finished with value: 0.7891391024573481 and parameters: {'n_estimators': 268, 'max_depth': 14, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:17,318] Trial 14 finished with value: 0.7906714876499291 and parameters: {'n_estimators': 374, 'max_depth': 15, 'min_samples_split': 9, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:20,206] Trial 16 finished with value: 0.7913020796250272 and parameters: {'n_estimators': 361, 'max_depth': 15, 'min_samples_split': 9, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:20,548] Trial 15 finished with value: 0.7905400601674234 and parameters: {'n_estimators': 379, 'max_depth': 15, 'min_samples_split': 9, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:24,502] Trial 17 finished with value: 0.7939812726103772 and parameters: {'n_estimators': 378, 'max_depth': 16, 'min_samples_split': 9, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:35,769] Trial 18 finished with value: 0.7933288758270038 and parameters: {'n_estimators': 387, 'max_depth': 16, 'min_samples_split': 8, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:36,918] Trial 21 finished with value: 0.7469577460787875 and parameters: {'n_estimators': 316, 'max_depth': 9, 'min_samples_split': 7, 'class_weight': None}. Best is trial 9 with value: 0.7963922495775568.\n[I 2025-05-01 00:44:41,918] Trial 19 finished with value: 0.7964071787945832 and parameters: {'n_estimators': 428, 'max_depth': 17, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:44:42,242] Trial 20 finished with value: 0.7962493516979252 and parameters: {'n_estimators': 427, 'max_depth': 17, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:44:48,702] Trial 23 finished with value: 0.7824870599882076 and parameters: {'n_estimators': 262, 'max_depth': 12, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:44:49,317] Trial 22 finished with value: 0.7714171921746793 and parameters: {'n_estimators': 312, 'max_depth': 11, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:00,809] Trial 24 finished with value: 0.7840636762099166 and parameters: {'n_estimators': 427, 'max_depth': 12, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:01,789] Trial 25 finished with value: 0.7848780926365156 and parameters: {'n_estimators': 440, 'max_depth': 12, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:10,739] Trial 26 finished with value: 0.796238022017616 and parameters: {'n_estimators': 434, 'max_depth': 17, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:10,927] Trial 27 finished with value: 0.796238022017616 and parameters: {'n_estimators': 433, 'max_depth': 17, 'min_samples_split': 3, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:14,309] Trial 29 finished with value: 0.7963380701685504 and parameters: {'n_estimators': 340, 'max_depth': 17, 'min_samples_split': 4, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:15,428] Trial 28 finished with value: 0.7961444789085083 and parameters: {'n_estimators': 430, 'max_depth': 17, 'min_samples_split': 4, 'class_weight': None}. Best is trial 19 with value: 0.7964071787945832.\n[I 2025-05-01 00:45:17,173] A new study created in memory with name: no-name-2ed1ad3b-cf6d-4158-83dc-a818f63e28b2\n[I 2025-05-01 00:45:47,216] Trial 2 finished with value: 0.7887051471589723 and parameters: {'n_estimators': 268, 'learning_rate': 0.09197647300549415, 'max_depth': 5, 'subsample': 0.71656823407965}. Best is trial 2 with value: 0.7887051471589723.\n[I 2025-05-01 00:45:58,412] Trial 1 finished with value: 0.7947772971168521 and parameters: {'n_estimators': 338, 'learning_rate': 0.2653280105292195, 'max_depth': 5, 'subsample': 0.7941031600650024}. Best is trial 1 with value: 0.7947772971168521.\n[I 2025-05-01 00:46:07,032] Trial 3 finished with value: 0.7917454786464273 and parameters: {'n_estimators': 136, 'learning_rate': 0.2808113099554353, 'max_depth': 10, 'subsample': 0.802361497572063}. Best is trial 1 with value: 0.7947772971168521.\n[I 2025-05-01 00:46:54,193] Trial 0 finished with value: 0.7961766688296266 and parameters: {'n_estimators': 343, 'learning_rate': 0.044059132086208604, 'max_depth': 8, 'subsample': 0.9923292506021862}. Best is trial 0 with value: 0.7961766688296266.\n[I 2025-05-01 00:47:23,892] Trial 7 finished with value: 0.7910964716741062 and parameters: {'n_estimators': 203, 'learning_rate': 0.2590560023398601, 'max_depth': 5, 'subsample': 0.9019536222879095}. Best is trial 0 with value: 0.7961766688296266.\n[I 2025-05-01 00:48:06,229] Trial 8 finished with value: 0.7965428384303939 and parameters: {'n_estimators': 103, 'learning_rate': 0.18939992593212085, 'max_depth': 9, 'subsample': 0.6475042214849185}. Best is trial 8 with value: 0.7965428384303939.\n[I 2025-05-01 00:48:27,648] Trial 6 finished with value: 0.7957957350460763 and parameters: {'n_estimators': 357, 'learning_rate': 0.05362057981156114, 'max_depth': 9, 'subsample': 0.7102540592292825}. Best is trial 8 with value: 0.7965428384303939.\n[I 2025-05-01 00:49:47,239] Trial 10 finished with value: 0.7967067648985455 and parameters: {'n_estimators': 221, 'learning_rate': 0.06361895665430842, 'max_depth': 8, 'subsample': 0.703442094002937}. Best is trial 10 with value: 0.7967067648985455.\n[I 2025-05-01 00:50:33,270] Trial 9 finished with value: 0.7901637973512059 and parameters: {'n_estimators': 244, 'learning_rate': 0.14912974538775547, 'max_depth': 11, 'subsample': 0.6664610571489287}. Best is trial 10 with value: 0.7967067648985455.\n[I 2025-05-01 00:52:17,309] Trial 12 finished with value: 0.7986176303440573 and parameters: {'n_estimators': 210, 'learning_rate': 0.04080560488669255, 'max_depth': 9, 'subsample': 0.7486176498256438}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:54:57,249] Trial 11 finished with value: 0.7881122054115212 and parameters: {'n_estimators': 236, 'learning_rate': 0.16391936397556298, 'max_depth': 15, 'subsample': 0.7594939382286894}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:54:58,167] Trial 5 finished with value: 0.7910113453845697 and parameters: {'n_estimators': 368, 'learning_rate': 0.011866818775908344, 'max_depth': 14, 'subsample': 0.9900535384881162}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:55:11,173] Trial 4 finished with value: 0.7964628983075157 and parameters: {'n_estimators': 500, 'learning_rate': 0.0389354611405432, 'max_depth': 14, 'subsample': 0.9893124906361274}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:55:55,280] Trial 16 finished with value: 0.7984348485793926 and parameters: {'n_estimators': 177, 'learning_rate': 0.10971902657421478, 'max_depth': 7, 'subsample': 0.606968227328528}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:56:14,540] Trial 17 finished with value: 0.7502208869120016 and parameters: {'n_estimators': 165, 'learning_rate': 0.13054245595908048, 'max_depth': 3, 'subsample': 0.622993557213642}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:57:15,578] Trial 15 finished with value: 0.796526003306969 and parameters: {'n_estimators': 488, 'learning_rate': 0.09788244196211952, 'max_depth': 7, 'subsample': 0.8576580704732591}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:58:52,847] Trial 18 finished with value: 0.7891157824924305 and parameters: {'n_estimators': 184, 'learning_rate': 0.11475040900671746, 'max_depth': 12, 'subsample': 0.8742198843513797}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 00:59:38,629] Trial 19 finished with value: 0.7858645419966596 and parameters: {'n_estimators': 177, 'learning_rate': 0.22075442676361687, 'max_depth': 12, 'subsample': 0.6096368694080379}. Best is trial 12 with value: 0.7986176303440573.\n[I 2025-05-01 01:00:16,280] Trial 20 finished with value: 0.800064542906074 and parameters: {'n_estimators': 290, 'learning_rate': 0.2221090965401421, 'max_depth': 7, 'subsample': 0.6171615182996689}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:00:48,147] Trial 21 finished with value: 0.7946091684644964 and parameters: {'n_estimators': 285, 'learning_rate': 0.08037652166960141, 'max_depth': 6, 'subsample': 0.7648444822882485}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:00:53,407] Trial 13 finished with value: 0.785133914919207 and parameters: {'n_estimators': 480, 'learning_rate': 0.1306961365124765, 'max_depth': 15, 'subsample': 0.6004910158691265}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:00:55,459] Trial 22 finished with value: 0.7735769798288417 and parameters: {'n_estimators': 291, 'learning_rate': 0.2227824820987996, 'max_depth': 3, 'subsample': 0.7639365653848084}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:01:46,116] Trial 23 finished with value: 0.7857822094383358 and parameters: {'n_estimators': 431, 'learning_rate': 0.2236119153755834, 'max_depth': 3, 'subsample': 0.6703725489411135}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:02:18,654] Trial 24 finished with value: 0.7991979563769407 and parameters: {'n_estimators': 314, 'learning_rate': 0.21649868423648005, 'max_depth': 7, 'subsample': 0.6559471269466326}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:02:21,086] Trial 14 finished with value: 0.7974138948091083 and parameters: {'n_estimators': 477, 'learning_rate': 0.016356376505879507, 'max_depth': 13, 'subsample': 0.6017897698794145}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:02:23,651] Trial 26 finished with value: 0.763320441209143 and parameters: {'n_estimators': 147, 'learning_rate': 0.012547505825497435, 'max_depth': 7, 'subsample': 0.6410166380922456}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:02:56,237] Trial 25 finished with value: 0.7970327503149033 and parameters: {'n_estimators': 402, 'learning_rate': 0.18600936573558063, 'max_depth': 8, 'subsample': 0.6700492206974393}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:03:20,614] Trial 27 finished with value: 0.7927014360630154 and parameters: {'n_estimators': 318, 'learning_rate': 0.18539113842044297, 'max_depth': 7, 'subsample': 0.6840837711785529}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:03:43,846] Trial 28 finished with value: 0.7941810621149855 and parameters: {'n_estimators': 324, 'learning_rate': 0.18752497946636157, 'max_depth': 10, 'subsample': 0.6734582753925127}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:03:51,464] Trial 29 finished with value: 0.7911442455895894 and parameters: {'n_estimators': 391, 'learning_rate': 0.19808755882591034, 'max_depth': 10, 'subsample': 0.6771791623107287}. Best is trial 20 with value: 0.800064542906074.\n[I 2025-05-01 01:03:57,423] A new study created in memory with name: no-name-27b96642-c77b-4915-946a-453a7cda7970\n[I 2025-05-01 01:04:08,281] Trial 2 finished with value: 0.807008716503202 and parameters: {'n_estimators': 200, 'max_depth': 19, 'min_samples_split': 7, 'class_weight': None}. Best is trial 2 with value: 0.807008716503202.\n[I 2025-05-01 01:04:12,812] Trial 1 finished with value: 0.7974611485728849 and parameters: {'n_estimators': 303, 'max_depth': 15, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.807008716503202.\n[I 2025-05-01 01:04:18,644] Trial 0 finished with value: 0.8092295012204399 and parameters: {'n_estimators': 397, 'max_depth': 17, 'min_samples_split': 4, 'class_weight': None}. Best is trial 0 with value: 0.8092295012204399.\n[I 2025-05-01 01:04:22,857] Trial 3 finished with value: 0.8094916074960729 and parameters: {'n_estimators': 477, 'max_depth': 16, 'min_samples_split': 2, 'class_weight': None}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:25,570] Trial 4 finished with value: 0.7815177341090431 and parameters: {'n_estimators': 361, 'max_depth': 11, 'min_samples_split': 7, 'class_weight': None}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:25,977] Trial 5 finished with value: 0.7297039350102882 and parameters: {'n_estimators': 319, 'max_depth': 6, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:28,097] Trial 7 finished with value: 0.654595922355057 and parameters: {'n_estimators': 127, 'max_depth': 3, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:36,375] Trial 8 finished with value: 0.7281727375541783 and parameters: {'n_estimators': 243, 'max_depth': 6, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:36,764] Trial 6 finished with value: 0.7791899266744678 and parameters: {'n_estimators': 377, 'max_depth': 10, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:39,975] Trial 9 finished with value: 0.6949427049268693 and parameters: {'n_estimators': 353, 'max_depth': 4, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:52,654] Trial 10 finished with value: 0.7838156350259489 and parameters: {'n_estimators': 490, 'max_depth': 12, 'min_samples_split': 10, 'class_weight': None}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:04:58,349] Trial 11 finished with value: 0.8082735126424657 and parameters: {'n_estimators': 392, 'max_depth': 19, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:05:01,029] Trial 12 finished with value: 0.8025365973687235 and parameters: {'n_estimators': 446, 'max_depth': 20, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:05:06,019] Trial 13 finished with value: 0.8082360147173449 and parameters: {'n_estimators': 494, 'max_depth': 14, 'min_samples_split': 2, 'class_weight': None}. Best is trial 3 with value: 0.8094916074960729.\n[I 2025-05-01 01:05:17,913] Trial 14 finished with value: 0.8115078863718409 and parameters: {'n_estimators': 475, 'max_depth': 17, 'min_samples_split': 2, 'class_weight': None}. Best is trial 14 with value: 0.8115078863718409.\n[I 2025-05-01 01:05:23,993] Trial 15 finished with value: 0.8102700296154355 and parameters: {'n_estimators': 483, 'max_depth': 16, 'min_samples_split': 2, 'class_weight': None}. Best is trial 14 with value: 0.8115078863718409.\n[I 2025-05-01 01:05:25,226] Trial 16 finished with value: 0.8104574980231997 and parameters: {'n_estimators': 459, 'max_depth': 16, 'min_samples_split': 2, 'class_weight': None}. Best is trial 14 with value: 0.8115078863718409.\n[I 2025-05-01 01:05:29,255] Trial 17 finished with value: 0.8046976544013074 and parameters: {'n_estimators': 433, 'max_depth': 17, 'min_samples_split': 5, 'class_weight': None}. Best is trial 14 with value: 0.8115078863718409.\n[I 2025-05-01 01:05:40,707] Trial 18 finished with value: 0.8118603188009148 and parameters: {'n_estimators': 432, 'max_depth': 16, 'min_samples_split': 2, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:05:41,822] Trial 21 finished with value: 0.802727585512055 and parameters: {'n_estimators': 258, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:05:47,501] Trial 19 finished with value: 0.8050977681468506 and parameters: {'n_estimators': 428, 'max_depth': 17, 'min_samples_split': 5, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:05:47,797] Trial 20 finished with value: 0.8008504222510598 and parameters: {'n_estimators': 441, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:06:01,777] Trial 23 finished with value: 0.7636174787747946 and parameters: {'n_estimators': 422, 'max_depth': 9, 'min_samples_split': 6, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:06:03,198] Trial 22 finished with value: 0.7996326496049349 and parameters: {'n_estimators': 418, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:06:07,294] Trial 25 finished with value: 0.7680077410832765 and parameters: {'n_estimators': 418, 'max_depth': 9, 'min_samples_split': 2, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:06:11,062] Trial 24 finished with value: 0.8078635953928872 and parameters: {'n_estimators': 448, 'max_depth': 14, 'min_samples_split': 2, 'class_weight': None}. Best is trial 18 with value: 0.8118603188009148.\n[I 2025-05-01 01:06:27,275] Trial 26 finished with value: 0.8121544300247552 and parameters: {'n_estimators': 462, 'max_depth': 18, 'min_samples_split': 2, 'class_weight': None}. Best is trial 26 with value: 0.8121544300247552.\n[I 2025-05-01 01:06:28,419] Trial 27 finished with value: 0.8069988157118099 and parameters: {'n_estimators': 463, 'max_depth': 19, 'min_samples_split': 2, 'class_weight': None}. Best is trial 26 with value: 0.8121544300247552.\n[I 2025-05-01 01:06:30,465] Trial 28 finished with value: 0.8093356419986527 and parameters: {'n_estimators': 465, 'max_depth': 18, 'min_samples_split': 3, 'class_weight': None}. Best is trial 26 with value: 0.8121544300247552.\n[I 2025-05-01 01:06:31,473] Trial 29 finished with value: 0.8098571731607398 and parameters: {'n_estimators': 468, 'max_depth': 18, 'min_samples_split': 3, 'class_weight': None}. Best is trial 26 with value: 0.8121544300247552.\n[I 2025-05-01 01:06:33,488] A new study created in memory with name: no-name-7407d33e-b95e-4c4f-85de-254296cf4e1a\n[I 2025-05-01 01:06:40,946] Trial 1 finished with value: 0.676599447889649 and parameters: {'n_estimators': 192, 'max_depth': 4, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.676599447889649.\n[I 2025-05-01 01:06:44,007] Trial 3 finished with value: 0.773320653583963 and parameters: {'n_estimators': 210, 'max_depth': 11, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.773320653583963.\n[I 2025-05-01 01:06:47,163] Trial 2 finished with value: 0.7477161603103736 and parameters: {'n_estimators': 282, 'max_depth': 9, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.773320653583963.\n[I 2025-05-01 01:06:49,646] Trial 4 finished with value: 0.7864703416453747 and parameters: {'n_estimators': 152, 'max_depth': 19, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.7864703416453747.\n[I 2025-05-01 01:06:56,106] Trial 6 finished with value: 0.7187235831011867 and parameters: {'n_estimators': 205, 'max_depth': 6, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.7864703416453747.\n[I 2025-05-01 01:07:00,667] Trial 0 finished with value: 0.7987070526891902 and parameters: {'n_estimators': 471, 'max_depth': 19, 'min_samples_split': 3, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:01,073] Trial 5 finished with value: 0.759004109746238 and parameters: {'n_estimators': 346, 'max_depth': 10, 'min_samples_split': 10, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:03,833] Trial 7 finished with value: 0.6481731036894673 and parameters: {'n_estimators': 374, 'max_depth': 3, 'min_samples_split': 3, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:07,212] Trial 8 finished with value: 0.7169902125612391 and parameters: {'n_estimators': 271, 'max_depth': 6, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:12,541] Trial 9 finished with value: 0.7349450869823368 and parameters: {'n_estimators': 265, 'max_depth': 8, 'min_samples_split': 2, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:19,720] Trial 12 finished with value: 0.72769020177321 and parameters: {'n_estimators': 295, 'max_depth': 7, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:20,068] Trial 10 finished with value: 0.7920703136630286 and parameters: {'n_estimators': 342, 'max_depth': 20, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:24,950] Trial 14 finished with value: 0.7857386977386979 and parameters: {'n_estimators': 101, 'max_depth': 20, 'min_samples_split': 11, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:25,003] Trial 11 finished with value: 0.7647693762163399 and parameters: {'n_estimators': 446, 'max_depth': 10, 'min_samples_split': 5, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:37,528] Trial 13 finished with value: 0.7949145696066323 and parameters: {'n_estimators': 458, 'max_depth': 20, 'min_samples_split': 4, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:47,313] Trial 15 finished with value: 0.7968422756580652 and parameters: {'n_estimators': 499, 'max_depth': 20, 'min_samples_split': 3, 'class_weight': None}. Best is trial 0 with value: 0.7987070526891902.\n[I 2025-05-01 01:07:50,594] Trial 16 finished with value: 0.7989229845078111 and parameters: {'n_estimators': 484, 'max_depth': 16, 'min_samples_split': 3, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:07:50,823] Trial 17 finished with value: 0.7968387158471664 and parameters: {'n_estimators': 491, 'max_depth': 16, 'min_samples_split': 2, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:04,453] Trial 18 finished with value: 0.7984634342539088 and parameters: {'n_estimators': 496, 'max_depth': 16, 'min_samples_split': 3, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:10,123] Trial 21 finished with value: 0.7833233511207619 and parameters: {'n_estimators': 401, 'max_depth': 14, 'min_samples_split': 8, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:10,902] Trial 20 finished with value: 0.7866827682170093 and parameters: {'n_estimators': 413, 'max_depth': 16, 'min_samples_split': 8, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:13,298] Trial 19 finished with value: 0.7984634342539088 and parameters: {'n_estimators': 496, 'max_depth': 16, 'min_samples_split': 3, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:24,608] Trial 22 finished with value: 0.7835991403455385 and parameters: {'n_estimators': 411, 'max_depth': 14, 'min_samples_split': 8, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:31,313] Trial 23 finished with value: 0.797667548505377 and parameters: {'n_estimators': 406, 'max_depth': 17, 'min_samples_split': 4, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:34,166] Trial 24 finished with value: 0.7983199125262526 and parameters: {'n_estimators': 448, 'max_depth': 17, 'min_samples_split': 4, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:35,027] Trial 25 finished with value: 0.7887302033666238 and parameters: {'n_estimators': 446, 'max_depth': 13, 'min_samples_split': 4, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:49,102] Trial 26 finished with value: 0.7960996849705768 and parameters: {'n_estimators': 454, 'max_depth': 18, 'min_samples_split': 4, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:52,695] Trial 27 finished with value: 0.7887742834325518 and parameters: {'n_estimators': 453, 'max_depth': 13, 'min_samples_split': 4, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:54,494] Trial 28 finished with value: 0.7936648681157363 and parameters: {'n_estimators': 463, 'max_depth': 13, 'min_samples_split': 3, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:55,419] Trial 29 finished with value: 0.7987427164552149 and parameters: {'n_estimators': 476, 'max_depth': 18, 'min_samples_split': 3, 'class_weight': None}. Best is trial 16 with value: 0.7989229845078111.\n[I 2025-05-01 01:08:57,375] A new study created in memory with name: no-name-bd8efb89-0d3a-4255-a489-e1fbeecf913d\n[I 2025-05-01 01:09:11,511] Trial 2 finished with value: 0.7742995563544308 and parameters: {'n_estimators': 272, 'max_depth': 18, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:12,263] Trial 3 finished with value: 0.7699865264379125 and parameters: {'n_estimators': 295, 'max_depth': 14, 'min_samples_split': 9, 'class_weight': None}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:20,415] Trial 1 finished with value: 0.7707309711999388 and parameters: {'n_estimators': 460, 'max_depth': 12, 'min_samples_split': 2, 'class_weight': None}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:21,004] Trial 0 finished with value: 0.7722007845343511 and parameters: {'n_estimators': 459, 'max_depth': 18, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:26,594] Trial 7 finished with value: 0.7728587253520478 and parameters: {'n_estimators': 110, 'max_depth': 17, 'min_samples_split': 8, 'class_weight': None}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:29,245] Trial 4 finished with value: 0.6710723125041231 and parameters: {'n_estimators': 463, 'max_depth': 4, 'min_samples_split': 7, 'class_weight': None}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:30,071] Trial 5 finished with value: 0.7360539890008738 and parameters: {'n_estimators': 387, 'max_depth': 9, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:36,890] Trial 8 finished with value: 0.6926335088688246 and parameters: {'n_estimators': 268, 'max_depth': 5, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7742995563544308.\n[I 2025-05-01 01:09:40,086] Trial 9 finished with value: 0.7759764185769746 and parameters: {'n_estimators': 208, 'max_depth': 17, 'min_samples_split': 9, 'class_weight': None}. Best is trial 9 with value: 0.7759764185769746.\n[I 2025-05-01 01:09:42,042] Trial 6 finished with value: 0.7520699655022469 and parameters: {'n_estimators': 451, 'max_depth': 11, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7759764185769746.\n[I 2025-05-01 01:09:50,778] Trial 11 finished with value: 0.6702207752248324 and parameters: {'n_estimators': 357, 'max_depth': 4, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7759764185769746.\n[I 2025-05-01 01:09:51,533] Trial 12 finished with value: 0.7246226843992795 and parameters: {'n_estimators': 236, 'max_depth': 8, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.7759764185769746.\n[I 2025-05-01 01:09:52,334] Trial 13 finished with value: 0.7758938620457848 and parameters: {'n_estimators': 167, 'max_depth': 20, 'min_samples_split': 5, 'class_weight': None}. Best is trial 9 with value: 0.7759764185769746.\n[I 2025-05-01 01:09:53,724] Trial 10 finished with value: 0.7799664975578249 and parameters: {'n_estimators': 433, 'max_depth': 16, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:00,011] Trial 16 finished with value: 0.7789660567216949 and parameters: {'n_estimators': 140, 'max_depth': 20, 'min_samples_split': 5, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:00,738] Trial 15 finished with value: 0.7724858426784309 and parameters: {'n_estimators': 186, 'max_depth': 20, 'min_samples_split': 11, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:01,245] Trial 14 finished with value: 0.7725779987689719 and parameters: {'n_estimators': 205, 'max_depth': 20, 'min_samples_split': 11, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:03,374] Trial 17 finished with value: 0.7779123176026157 and parameters: {'n_estimators': 184, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:19,050] Trial 18 finished with value: 0.7788034984380992 and parameters: {'n_estimators': 364, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:19,323] Trial 20 finished with value: 0.7790640079790869 and parameters: {'n_estimators': 352, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:19,545] Trial 19 finished with value: 0.7793419734172228 and parameters: {'n_estimators': 360, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:21,515] Trial 21 finished with value: 0.7785076797454291 and parameters: {'n_estimators': 351, 'max_depth': 15, 'min_samples_split': 4, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:25,261] Trial 22 finished with value: 0.7721004703639822 and parameters: {'n_estimators': 125, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:38,756] Trial 24 finished with value: 0.7786079069098983 and parameters: {'n_estimators': 390, 'max_depth': 13, 'min_samples_split': 3, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:40,711] Trial 23 finished with value: 0.7791815200602306 and parameters: {'n_estimators': 414, 'max_depth': 13, 'min_samples_split': 3, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:42,138] Trial 25 finished with value: 0.7783378048715572 and parameters: {'n_estimators': 409, 'max_depth': 13, 'min_samples_split': 3, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:45,209] Trial 26 finished with value: 0.759709934190495 and parameters: {'n_estimators': 412, 'max_depth': 11, 'min_samples_split': 3, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:10:56,222] Trial 27 finished with value: 0.7446606054317065 and parameters: {'n_estimators': 420, 'max_depth': 10, 'min_samples_split': 6, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:11:00,267] Trial 28 finished with value: 0.7460233866597323 and parameters: {'n_estimators': 497, 'max_depth': 10, 'min_samples_split': 3, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:11:00,647] Trial 29 finished with value: 0.7447309251207269 and parameters: {'n_estimators': 499, 'max_depth': 10, 'min_samples_split': 6, 'class_weight': None}. Best is trial 10 with value: 0.7799664975578249.\n[I 2025-05-01 01:11:02,334] A new study created in memory with name: no-name-5325a6a1-86d5-4802-8c0c-d61430f2f70a\n[I 2025-05-01 01:11:13,489] Trial 3 finished with value: 0.7590450394172595 and parameters: {'n_estimators': 229, 'max_depth': 17, 'min_samples_split': 7, 'class_weight': None}. Best is trial 3 with value: 0.7590450394172595.\n[I 2025-05-01 01:11:17,300] Trial 1 finished with value: 0.7577331874232214 and parameters: {'n_estimators': 289, 'max_depth': 17, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7590450394172595.\n[I 2025-05-01 01:11:20,745] Trial 0 finished with value: 0.7576518366067442 and parameters: {'n_estimators': 377, 'max_depth': 15, 'min_samples_split': 8, 'class_weight': None}. Best is trial 3 with value: 0.7590450394172595.\n[I 2025-05-01 01:11:23,114] Trial 2 finished with value: 0.7607192583609738 and parameters: {'n_estimators': 410, 'max_depth': 16, 'min_samples_split': 6, 'class_weight': None}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:29,787] Trial 7 finished with value: 0.7093181989052764 and parameters: {'n_estimators': 155, 'max_depth': 6, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:32,537] Trial 4 finished with value: 0.7214875935853183 and parameters: {'n_estimators': 450, 'max_depth': 8, 'min_samples_split': 8, 'class_weight': None}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:35,403] Trial 8 finished with value: 0.7569363945122254 and parameters: {'n_estimators': 115, 'max_depth': 20, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:37,977] Trial 5 finished with value: 0.724122232291655 and parameters: {'n_estimators': 466, 'max_depth': 8, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:40,346] Trial 6 finished with value: 0.7575818759349813 and parameters: {'n_estimators': 365, 'max_depth': 16, 'min_samples_split': 8, 'class_weight': None}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:42,330] Trial 9 finished with value: 0.7582729673986115 and parameters: {'n_estimators': 174, 'max_depth': 18, 'min_samples_split': 7, 'class_weight': None}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:51,742] Trial 10 finished with value: 0.7288304428674628 and parameters: {'n_estimators': 325, 'max_depth': 9, 'min_samples_split': 8, 'class_weight': None}. Best is trial 2 with value: 0.7607192583609738.\n[I 2025-05-01 01:11:54,037] Trial 11 finished with value: 0.7633616446479279 and parameters: {'n_estimators': 290, 'max_depth': 15, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 11 with value: 0.7633616446479279.\n[I 2025-05-01 01:12:01,229] Trial 13 finished with value: 0.7576051722020264 and parameters: {'n_estimators': 374, 'max_depth': 12, 'min_samples_split': 4, 'class_weight': None}. Best is trial 11 with value: 0.7633616446479279.\n[I 2025-05-01 01:12:03,242] Trial 14 finished with value: 0.757331058371364 and parameters: {'n_estimators': 241, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': None}. Best is trial 11 with value: 0.7633616446479279.\n[I 2025-05-01 01:12:04,696] Trial 12 finished with value: 0.7585880607341382 and parameters: {'n_estimators': 490, 'max_depth': 12, 'min_samples_split': 4, 'class_weight': None}. Best is trial 11 with value: 0.7633616446479279.\n[I 2025-05-01 01:12:06,448] Trial 15 finished with value: 0.7670497613155557 and parameters: {'n_estimators': 249, 'max_depth': 13, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:13,651] Trial 16 finished with value: 0.758579507333534 and parameters: {'n_estimators': 245, 'max_depth': 13, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:16,453] Trial 19 finished with value: 0.6433957153346329 and parameters: {'n_estimators': 269, 'max_depth': 3, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:19,052] Trial 17 finished with value: 0.6445844507770337 and parameters: {'n_estimators': 420, 'max_depth': 3, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:24,819] Trial 20 finished with value: 0.6427768283554851 and parameters: {'n_estimators': 303, 'max_depth': 3, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:25,836] Trial 21 finished with value: 0.7407511503666007 and parameters: {'n_estimators': 195, 'max_depth': 10, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:27,425] Trial 18 finished with value: 0.7639458684387179 and parameters: {'n_estimators': 429, 'max_depth': 14, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:28,002] Trial 22 finished with value: 0.7425277415299549 and parameters: {'n_estimators': 195, 'max_depth': 10, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:34,530] Trial 23 finished with value: 0.7642954040206815 and parameters: {'n_estimators': 197, 'max_depth': 14, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:41,137] Trial 27 finished with value: 0.7656628875967415 and parameters: {'n_estimators': 130, 'max_depth': 14, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:42,070] Trial 24 finished with value: 0.7633548278241749 and parameters: {'n_estimators': 337, 'max_depth': 14, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:43,905] Trial 26 finished with value: 0.766360104871827 and parameters: {'n_estimators': 329, 'max_depth': 14, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 15 with value: 0.7670497613155557.\n[I 2025-05-01 01:12:43,952] Trial 25 finished with value: 0.7675974763586783 and parameters: {'n_estimators': 334, 'max_depth': 14, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.7675974763586783.\n[I 2025-05-01 01:12:45,598] Trial 29 finished with value: 0.7488544307707994 and parameters: {'n_estimators': 111, 'max_depth': 11, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.7675974763586783.\n[I 2025-05-01 01:12:45,685] Trial 28 finished with value: 0.7586163064600651 and parameters: {'n_estimators': 112, 'max_depth': 19, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.7675974763586783.\n[I 2025-05-01 01:12:47,495] A new study created in memory with name: no-name-85ad9233-1f93-49db-ab08-28cf4b7e6f41\n[I 2025-05-01 01:12:53,936] Trial 3 finished with value: 0.6881710521021336 and parameters: {'n_estimators': 121, 'max_depth': 5, 'min_samples_split': 9, 'class_weight': None}. Best is trial 3 with value: 0.6881710521021336.\n[I 2025-05-01 01:13:00,187] Trial 1 finished with value: 0.7538896103128943 and parameters: {'n_estimators': 170, 'max_depth': 13, 'min_samples_split': 10, 'class_weight': None}. Best is trial 1 with value: 0.7538896103128943.\n[I 2025-05-01 01:13:03,830] Trial 0 finished with value: 0.7312247949942969 and parameters: {'n_estimators': 254, 'max_depth': 9, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.7538896103128943.\n[I 2025-05-01 01:13:13,335] Trial 2 finished with value: 0.7679468584901461 and parameters: {'n_estimators': 378, 'max_depth': 18, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7679468584901461.\n[I 2025-05-01 01:13:15,349] Trial 4 finished with value: 0.7693527024593438 and parameters: {'n_estimators': 337, 'max_depth': 20, 'min_samples_split': 5, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:19,750] Trial 5 finished with value: 0.7556671432460764 and parameters: {'n_estimators': 375, 'max_depth': 14, 'min_samples_split': 11, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:28,162] Trial 6 finished with value: 0.7665017907395925 and parameters: {'n_estimators': 463, 'max_depth': 17, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:30,063] Trial 7 finished with value: 0.7659976988945135 and parameters: {'n_estimators': 315, 'max_depth': 15, 'min_samples_split': 8, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:32,319] Trial 9 finished with value: 0.7084812337430549 and parameters: {'n_estimators': 277, 'max_depth': 7, 'min_samples_split': 10, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:32,995] Trial 8 finished with value: 0.7428491397698354 and parameters: {'n_estimators': 343, 'max_depth': 10, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:35,649] Trial 11 finished with value: 0.7613296673180473 and parameters: {'n_estimators': 108, 'max_depth': 12, 'min_samples_split': 5, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:42,196] Trial 10 finished with value: 0.7014386139624057 and parameters: {'n_estimators': 327, 'max_depth': 6, 'min_samples_split': 9, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:52,038] Trial 12 finished with value: 0.7471662454935133 and parameters: {'n_estimators': 407, 'max_depth': 10, 'min_samples_split': 2, 'class_weight': None}. Best is trial 4 with value: 0.7693527024593438.\n[I 2025-05-01 01:13:58,648] Trial 13 finished with value: 0.7703132237633034 and parameters: {'n_estimators': 475, 'max_depth': 20, 'min_samples_split': 6, 'class_weight': None}. Best is trial 13 with value: 0.7703132237633034.\n[I 2025-05-01 01:13:59,322] Trial 14 finished with value: 0.7708483738778207 and parameters: {'n_estimators': 431, 'max_depth': 20, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 14 with value: 0.7708483738778207.\n[I 2025-05-01 01:14:05,495] Trial 15 finished with value: 0.7697781002106375 and parameters: {'n_estimators': 424, 'max_depth': 20, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 14 with value: 0.7708483738778207.\n[I 2025-05-01 01:14:16,785] Trial 16 finished with value: 0.7699725953509696 and parameters: {'n_estimators': 481, 'max_depth': 20, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 14 with value: 0.7708483738778207.\n[I 2025-05-01 01:14:23,820] Trial 17 finished with value: 0.7689536898709108 and parameters: {'n_estimators': 486, 'max_depth': 20, 'min_samples_split': 6, 'class_weight': None}. Best is trial 14 with value: 0.7708483738778207.\n[I 2025-05-01 01:14:24,607] Trial 18 finished with value: 0.7712567585812204 and parameters: {'n_estimators': 495, 'max_depth': 20, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:14:28,383] Trial 19 finished with value: 0.7672054308725025 and parameters: {'n_estimators': 449, 'max_depth': 16, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:14:40,964] Trial 22 finished with value: 0.6589814950319826 and parameters: {'n_estimators': 433, 'max_depth': 3, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:14:43,472] Trial 20 finished with value: 0.7690115163754339 and parameters: {'n_estimators': 499, 'max_depth': 17, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:14:44,232] Trial 23 finished with value: 0.6589814950319826 and parameters: {'n_estimators': 433, 'max_depth': 3, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:14:47,143] Trial 21 finished with value: 0.7688326306243248 and parameters: {'n_estimators': 433, 'max_depth': 17, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:04,443] Trial 25 finished with value: 0.7661685485192677 and parameters: {'n_estimators': 394, 'max_depth': 18, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:05,037] Trial 26 finished with value: 0.7676287403605137 and parameters: {'n_estimators': 389, 'max_depth': 18, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:07,573] Trial 24 finished with value: 0.7651086233560174 and parameters: {'n_estimators': 496, 'max_depth': 18, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:12,020] Trial 27 finished with value: 0.7702017916868382 and parameters: {'n_estimators': 500, 'max_depth': 18, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:13,205] Trial 28 finished with value: 0.763760029885659 and parameters: {'n_estimators': 232, 'max_depth': 19, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:17,216] Trial 29 finished with value: 0.7665755297442742 and parameters: {'n_estimators': 458, 'max_depth': 19, 'min_samples_split': 8, 'class_weight': None}. Best is trial 18 with value: 0.7712567585812204.\n[I 2025-05-01 01:15:19,265] A new study created in memory with name: no-name-15139b29-84a7-407b-90b7-120e77b1bd62\n[I 2025-05-01 01:16:01,917] Trial 3 finished with value: 0.8265546248277751 and parameters: {'n_estimators': 274, 'learning_rate': 0.17277099363280649, 'max_depth': 5, 'subsample': 0.9377885047031796}. Best is trial 3 with value: 0.8265546248277751.\n[I 2025-05-01 01:16:22,549] Trial 4 finished with value: 0.7657172263224223 and parameters: {'n_estimators': 193, 'learning_rate': 0.07875757652586259, 'max_depth': 3, 'subsample': 0.8080902744562566}. Best is trial 3 with value: 0.8265546248277751.\n[I 2025-05-01 01:16:32,750] Trial 0 finished with value: 0.8199393796659831 and parameters: {'n_estimators': 261, 'learning_rate': 0.17872769436156866, 'max_depth': 9, 'subsample': 0.718361184986503}. Best is trial 3 with value: 0.8265546248277751.\n[I 2025-05-01 01:16:38,220] Trial 1 finished with value: 0.8278288270373426 and parameters: {'n_estimators': 369, 'learning_rate': 0.0881459919781947, 'max_depth': 7, 'subsample': 0.7827165273947957}. Best is trial 1 with value: 0.8278288270373426.\n[I 2025-05-01 01:17:26,729] Trial 2 finished with value: 0.8118665665732878 and parameters: {'n_estimators': 166, 'learning_rate': 0.12231724856589646, 'max_depth': 14, 'subsample': 0.8230079717749841}. Best is trial 1 with value: 0.8278288270373426.\n[I 2025-05-01 01:18:37,521] Trial 6 finished with value: 0.8193519733555924 and parameters: {'n_estimators': 322, 'learning_rate': 0.02503052711520039, 'max_depth': 9, 'subsample': 0.8239208586405177}. Best is trial 1 with value: 0.8278288270373426.\n[I 2025-05-01 01:18:43,789] Trial 8 finished with value: 0.8293386522113713 and parameters: {'n_estimators': 317, 'learning_rate': 0.06940525176455331, 'max_depth': 7, 'subsample': 0.8209388645283446}. Best is trial 8 with value: 0.8293386522113713.\n[I 2025-05-01 01:18:51,216] Trial 5 finished with value: 0.8207276773263856 and parameters: {'n_estimators': 341, 'learning_rate': 0.023225202205129886, 'max_depth': 10, 'subsample': 0.6565248487348609}. Best is trial 8 with value: 0.8293386522113713.\n[I 2025-05-01 01:19:01,498] Trial 7 finished with value: 0.8102918961910387 and parameters: {'n_estimators': 197, 'learning_rate': 0.2721484044812199, 'max_depth': 14, 'subsample': 0.9011397657647136}. Best is trial 8 with value: 0.8293386522113713.\n[I 2025-05-01 01:19:37,248] Trial 10 finished with value: 0.835462728623213 and parameters: {'n_estimators': 455, 'learning_rate': 0.25279434605536244, 'max_depth': 4, 'subsample': 0.9772446439440851}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:19:41,154] Trial 9 finished with value: 0.8245108771188635 and parameters: {'n_estimators': 470, 'learning_rate': 0.10513521478659216, 'max_depth': 5, 'subsample': 0.6854724986021641}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:19:42,249] Trial 11 finished with value: 0.805133708520887 and parameters: {'n_estimators': 129, 'learning_rate': 0.022636814336496447, 'max_depth': 10, 'subsample': 0.781797500905778}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:20:22,334] Trial 14 finished with value: 0.8259306989704103 and parameters: {'n_estimators': 472, 'learning_rate': 0.27514391864992244, 'max_depth': 3, 'subsample': 0.9848960175892585}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:20:26,463] Trial 15 finished with value: 0.8301456950084415 and parameters: {'n_estimators': 493, 'learning_rate': 0.29483967460628074, 'max_depth': 3, 'subsample': 0.9913139659214736}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:20:34,289] Trial 13 finished with value: 0.8353476156028398 and parameters: {'n_estimators': 491, 'learning_rate': 0.29758214127537497, 'max_depth': 4, 'subsample': 0.9938925920052691}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:20:58,383] Trial 12 finished with value: 0.8089757656912463 and parameters: {'n_estimators': 362, 'learning_rate': 0.29704359359600113, 'max_depth': 10, 'subsample': 0.9512385224716622}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:21:14,883] Trial 16 finished with value: 0.8193177552481551 and parameters: {'n_estimators': 409, 'learning_rate': 0.2232042569961139, 'max_depth': 6, 'subsample': 0.6056660208827773}. Best is trial 10 with value: 0.835462728623213.\n[I 2025-05-01 01:21:18,046] Trial 17 finished with value: 0.8380475659444286 and parameters: {'n_estimators': 417, 'learning_rate': 0.22383023575447247, 'max_depth': 5, 'subsample': 0.9876263013291382}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:21:20,952] Trial 18 finished with value: 0.8287187043460005 and parameters: {'n_estimators': 413, 'learning_rate': 0.22509201653517194, 'max_depth': 5, 'subsample': 0.8968664714023408}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:21:42,745] Trial 19 finished with value: 0.8317882268017855 and parameters: {'n_estimators': 413, 'learning_rate': 0.22931496604712243, 'max_depth': 5, 'subsample': 0.8818703988553993}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:22:12,809] Trial 20 finished with value: 0.8305557783514412 and parameters: {'n_estimators': 433, 'learning_rate': 0.23625944281723021, 'max_depth': 5, 'subsample': 0.8915755161533173}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:22:42,159] Trial 21 finished with value: 0.827652348946558 and parameters: {'n_estimators': 415, 'learning_rate': 0.22378453543173216, 'max_depth': 7, 'subsample': 0.8810869104297693}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:22:46,414] Trial 22 finished with value: 0.8228611064831293 and parameters: {'n_estimators': 425, 'learning_rate': 0.22345941505151454, 'max_depth': 7, 'subsample': 0.8759972618960027}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:23:05,228] Trial 24 finished with value: 0.8366672697232594 and parameters: {'n_estimators': 449, 'learning_rate': 0.2612089498323994, 'max_depth': 4, 'subsample': 0.9525716006281946}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:23:27,017] Trial 25 finished with value: 0.837258473693599 and parameters: {'n_estimators': 452, 'learning_rate': 0.2582599737650069, 'max_depth': 4, 'subsample': 0.949475416714515}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:23:35,452] Trial 26 finished with value: 0.8351510609517188 and parameters: {'n_estimators': 500, 'learning_rate': 0.2639203239050095, 'max_depth': 4, 'subsample': 0.9616284330298448}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:23:50,932] Trial 27 finished with value: 0.833047263562895 and parameters: {'n_estimators': 451, 'learning_rate': 0.19392820087213933, 'max_depth': 4, 'subsample': 0.9446780147139031}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:24:07,231] Trial 28 finished with value: 0.8280538036881033 and parameters: {'n_estimators': 379, 'learning_rate': 0.19271282247475477, 'max_depth': 4, 'subsample': 0.9286687578440426}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:24:30,158] Trial 23 finished with value: 0.8062018134011751 and parameters: {'n_estimators': 442, 'learning_rate': 0.2484775882315975, 'max_depth': 12, 'subsample': 0.930851894868193}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:24:58,493] Trial 29 finished with value: 0.81364670984102 and parameters: {'n_estimators': 375, 'learning_rate': 0.18686737624433197, 'max_depth': 12, 'subsample': 0.9294509818447775}. Best is trial 17 with value: 0.8380475659444286.\n[I 2025-05-01 01:25:02,418] A new study created in memory with name: no-name-8fc7e08c-5b7a-4b6b-a5d2-055e703891e8\n[I 2025-05-01 01:25:16,844] Trial 3 finished with value: 0.7777928168299848 and parameters: {'n_estimators': 101, 'learning_rate': 0.14689285414910344, 'max_depth': 5, 'subsample': 0.6042714443752251}. Best is trial 3 with value: 0.7777928168299848.\n[I 2025-05-01 01:25:44,739] Trial 0 finished with value: 0.7931280130916604 and parameters: {'n_estimators': 114, 'learning_rate': 0.26104531513338497, 'max_depth': 10, 'subsample': 0.7541169932022924}. Best is trial 0 with value: 0.7931280130916604.\n[I 2025-05-01 01:25:54,506] Trial 4 finished with value: 0.7879587225269716 and parameters: {'n_estimators': 159, 'learning_rate': 0.047179494107078404, 'max_depth': 7, 'subsample': 0.9859705587265308}. Best is trial 0 with value: 0.7931280130916604.\n[I 2025-05-01 01:26:24,615] Trial 1 finished with value: 0.8020760088697949 and parameters: {'n_estimators': 292, 'learning_rate': 0.059310877239577814, 'max_depth': 8, 'subsample': 0.8971365245300593}. Best is trial 1 with value: 0.8020760088697949.\n[I 2025-05-01 01:27:18,283] Trial 7 finished with value: 0.7998152845225462 and parameters: {'n_estimators': 305, 'learning_rate': 0.29873768051046745, 'max_depth': 6, 'subsample': 0.6185799084604887}. Best is trial 1 with value: 0.8020760088697949.\n[I 2025-05-01 01:28:27,038] Trial 6 finished with value: 0.7960372256122155 and parameters: {'n_estimators': 438, 'learning_rate': 0.14963240004256456, 'max_depth': 9, 'subsample': 0.9325975198745353}. Best is trial 1 with value: 0.8020760088697949.\n[I 2025-05-01 01:28:52,671] Trial 2 finished with value: 0.7805887125385023 and parameters: {'n_estimators': 390, 'learning_rate': 0.28002906879907224, 'max_depth': 15, 'subsample': 0.8809767986493203}. Best is trial 1 with value: 0.8020760088697949.\n[I 2025-05-01 01:29:42,782] Trial 8 finished with value: 0.7933093878327144 and parameters: {'n_estimators': 162, 'learning_rate': 0.16733725441869549, 'max_depth': 14, 'subsample': 0.8040872954366274}. Best is trial 1 with value: 0.8020760088697949.\n[I 2025-05-01 01:29:45,342] Trial 9 finished with value: 0.8030506385162338 and parameters: {'n_estimators': 137, 'learning_rate': 0.12719291078240635, 'max_depth': 11, 'subsample': 0.7881255057579413}. Best is trial 9 with value: 0.8030506385162338.\n[I 2025-05-01 01:30:07,048] Trial 11 finished with value: 0.7920153204522073 and parameters: {'n_estimators': 119, 'learning_rate': 0.17308574258640994, 'max_depth': 5, 'subsample': 0.9856472930965191}. Best is trial 9 with value: 0.8030506385162338.\n[I 2025-05-01 01:30:28,165] Trial 5 finished with value: 0.7979704081347363 and parameters: {'n_estimators': 384, 'learning_rate': 0.11328798979445326, 'max_depth': 13, 'subsample': 0.9906625336591228}. Best is trial 9 with value: 0.8030506385162338.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2fd847da-3e6b-4a40-b832-b22dec3d95b9","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"daf473a360634f1eb02ebe6e6e2e975e","deepnote_cell_type":"code"},"source":"# ========= 1. Импорты и Настройки =========\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nfrom joblib import Memory\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (accuracy_score, balanced_accuracy_score, cohen_kappa_score,\n                             f1_score, matthews_corrcoef, log_loss, roc_auc_score,\n                             average_precision_score, make_scorer, confusion_matrix)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import Pipeline as SklearnPipeline\nfrom sklearn.base import clone\nfrom sklearn.impute import KNNImputer\nfrom scipy.sparse import issparse\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom optuna.integration import OptunaSearchCV\nfrom optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution\nimport shap\nimport json\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Конфигурация\nDATA_PATH = 'skempi_v2.csv'\nLOCATION_TO_ANALYZE = 'Other'\nTEST_SIZE = 0.25\nRANDOM_STATE = 42\nCV_FOLDS = 5\nN_FEATURES_TO_SELECT = 30\n\n# ========= 2. Расширенная Предобработка =========\ndef enhanced_preprocessing(df):\n    \"\"\"Добавление структурных и эволюционных признаков\"\"\"\n    # Добавление структурных признаков (гипотетических значений)\n    df['delta_rsa'] = np.random.normal(0, 1, len(df))  # ΔSASA\n    df['conservation_score'] = np.random.normal(0.5, 0.2, len(df))  # Консервативность\n    \n    # Расширенные физико-химические свойства\n    aa_properties = {\n        'A': {'volume': 88.6, 'hydrophobicity': 1.8, 'polarity': 0},\n        'R': {'volume': 173.4, 'hydrophobicity': -4.5, 'polarity': 1},\n        'N': {'volume': 114.1, 'hydrophobicity': -3.5, 'polarity': 1},\n        'D': {'volume': 111.1, 'hydrophobicity': -3.5, 'polarity': 1},\n        'C': {'volume': 108.5, 'hydrophobicity': 2.5, 'polarity': 0},\n        'Q': {'volume': 143.8, 'hydrophobicity': -3.5, 'polarity': 1},\n        'E': {'volume': 138.4, 'hydrophobicity': -3.5, 'polarity': 1},\n        'G': {'volume': 60.1, 'hydrophobicity': -0.4, 'polarity': 0},\n        'H': {'volume': 153.2, 'hydrophobicity': -3.2, 'polarity': 1},\n        'I': {'volume': 166.7, 'hydrophobicity': 4.5, 'polarity': 0},\n        'L': {'volume': 166.7, 'hydrophobicity': 3.8, 'polarity': 0},\n        'K': {'volume': 168.6, 'hydrophobicity': -3.9, 'polarity': 1},\n        'M': {'volume': 162.9, 'hydrophobicity': 1.9, 'polarity': 0},\n        'F': {'volume': 189.9, 'hydrophobicity': 2.8, 'polarity': 0},\n        'P': {'volume': 112.7, 'hydrophobicity': -1.6, 'polarity': 0},\n        'S': {'volume': 89.0, 'hydrophobicity': -0.8, 'polarity': 1},\n        'T': {'volume': 116.1, 'hydrophobicity': -0.7, 'polarity': 1},\n        'W': {'volume': 227.8, 'hydrophobicity': -0.9, 'polarity': 0},\n        'Y': {'volume': 193.6, 'hydrophobicity': -1.3, 'polarity': 1},\n        'V': {'volume': 140.0, 'hydrophobicity': 4.2, 'polarity': 0}\n    }\n    \n    # Добавление новых признаков\n    df['charge_diff'] = df['MUT_AA'].map(lambda x: 1 if x in 'KR' else (-1 if x in 'DE' else 0)) - \\\n                        df['WT_AA'].map(lambda x: 1 if x in 'KR' else (-1 if x in 'DE' else 0))\n    \n    return df\n\n# ========= 3. Загрузка и Подготовка Данных =========\ndef load_and_preprocess_data(data_path):\n    \"\"\"Загружает и выполняет базовую предобработку данных SKEMPI с отладочными сообщениями.\"\"\"\n    print(f\"\\n--- [DEBUG] Загрузка данных из: {data_path} ---\")\n    if not os.path.exists(data_path):\n        raise FileNotFoundError(f\"Файл {data_path} не найден.\")\n    \n    expected_column_names = ['#Pdb', 'Mutation(s)_PDB', 'Mutation(s)_cleaned', 'iMutation_Location(s)', 'Hold_out_type', 'Hold_out_proteins', 'Affinity_mut (M)', 'Affinity_mut_parsed', 'Affinity_wt (M)', 'Affinity_wt_parsed', 'Reference', 'Protein 1', 'Protein 2', 'Temperature', 'kon_mut (M^(-1)s^(-1))', 'kon_mut_parsed', 'kon_wt (M^(-1)s^(-1))', 'kon_wt_parsed', 'koff_mut (s^(-1))', 'koff_mut_parsed', 'koff_wt (s^(-1))', 'koff_wt_parsed', 'dH_mut (kcal mol^(-1))', 'dH_wt (kcal mol^(-1))', 'dS_mut (cal mol^(-1) K^(-1))', 'dS_wt (cal mol^(-1) K^(-1))', 'Notes', 'Method', 'SKEMPI version']\n    \n    try:\n        print(\"[DEBUG] Попытка чтения с header=0, sep=';'...\")\n        df = pd.read_csv(data_path, sep=';', header=0, low_memory=False)\n        print(f\"[DEBUG] Размер после загрузки (header=0): {df.shape}\")\n        \n        if len(df.columns) != len(expected_column_names):\n            print(f\"[DEBUG] ВНИМАНИЕ: Кол-во колонок ({len(df.columns)}) не совпадает с ожидаемым ({len(expected_column_names)}). Переименовываем...\")\n            if len(df.columns) < len(expected_column_names):\n                df.columns = expected_column_names[:len(df.columns)]\n            else:\n                df.columns = expected_column_names + [f'extra_{i}' for i in range(len(df.columns) - len(expected_column_names))]\n        else:\n            df.columns = expected_column_names\n            print(\"[DEBUG] Колонки переименованы в стандартные.\")\n    except Exception as e:\n        print(f\"[DEBUG] Ошибка при чтении CSV: {e}\")\n        try:\n            print(\"[DEBUG] Попытка чтения без header, с присвоением имен...\")\n            df = pd.read_csv(data_path, sep=';', header=None, names=expected_column_names, low_memory=False)\n            print(f\"[DEBUG] Размер после загрузки (header=None): {df.shape}\")\n            if df.iloc[0]['#Pdb'] == '#Pdb':\n                df = df.iloc[1:].reset_index(drop=True)\n                print(\"[DEBUG] Первая строка, похожая на заголовок, удалена.\")\n        except Exception as e2:\n            print(f\"[DEBUG] Вторая ошибка при чтении CSV: {e2}\")\n            return pd.DataFrame(), [], []\n    \n    if df.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст сразу после загрузки.\")\n        return pd.DataFrame(), [], []\n    \n    if 'Mutation(s)_cleaned' not in df.columns:\n        print(\"[DEBUG] ОШИБКА: Столбец 'Mutation(s)_cleaned' не найден после загрузки/переименования.\")\n        return pd.DataFrame(), [], []\n    \n    original_rows = df.shape[0]\n    print(f\"[DEBUG] Начальный размер перед фильтрацией мутаций: {original_rows}\")\n    \n    df = df.dropna(subset=['Mutation(s)_cleaned'])\n    print(f\"[DEBUG] Размер после dropna('Mutation(s)_cleaned'): {df.shape} (удалено {original_rows - df.shape[0]})\")\n    \n    if df.empty: return pd.DataFrame(), [], []\n    \n    df['WT_AA'] = df['Mutation(s)_cleaned'].astype(str).str[0]\n    df['MUT_AA'] = df['Mutation(s)_cleaned'].astype(str).str[-1]\n    print(\"[DEBUG] WT_AA и MUT_AA извлечены.\")\n    \n    aa_properties = { 'A': {'volume': 88.6, 'hydrophobicity': 1.8}, 'R': {'volume': 173.4, 'hydrophobicity': -4.5}, 'N': {'volume': 114.1, 'hydrophobicity': -3.5}, 'D': {'volume': 111.1, 'hydrophobicity': -3.5}, 'C': {'volume': 108.5, 'hydrophobicity': 2.5}, 'Q': {'volume': 143.8, 'hydrophobicity': -3.5}, 'E': {'volume': 138.4, 'hydrophobicity': -3.5}, 'G': {'volume': 60.1, 'hydrophobicity': -0.4}, 'H': {'volume': 153.2, 'hydrophobicity': -3.2}, 'I': {'volume': 166.7, 'hydrophobicity': 4.5}, 'L': {'volume': 166.7, 'hydrophobicity': 3.8}, 'K': {'volume': 168.6, 'hydrophobicity': -3.9}, 'M': {'volume': 162.9, 'hydrophobicity': 1.9}, 'F': {'volume': 189.9, 'hydrophobicity': 2.8}, 'P': {'volume': 112.7, 'hydrophobicity': -1.6}, 'S': {'volume': 89.0, 'hydrophobicity': -0.8}, 'T': {'volume': 116.1, 'hydrophobicity': -0.7}, 'W': {'volume': 227.8, 'hydrophobicity': -0.9}, 'Y': {'volume': 193.6, 'hydrophobicity': -1.3}, 'V': {'volume': 140.0, 'hydrophobicity': 4.2}}\n    \n    valid_aa = set(aa_properties.keys())\n    original_rows_aa_filter = df.shape[0]\n    df = df[df['WT_AA'].isin(valid_aa) & df['MUT_AA'].isin(valid_aa)]\n    print(f\"[DEBUG] Размер после фильтрации невалидных WT/MUT AA: {df.shape} (удалено {original_rows_aa_filter - df.shape[0]})\")\n    \n    if df.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст после фильтрации невалидных WT/MUT AA.\")\n        return pd.DataFrame(), [], []\n    \n    original_rows_self_mut = df.shape[0]\n    df = df[df['WT_AA'] != df['MUT_AA']]\n    print(f\"[DEBUG] Размер после удаления мутаций A->A: {df.shape} (удалено {original_rows_self_mut - df.shape[0]})\")\n    \n    if df.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст после удаления мутаций A->A.\")\n        return pd.DataFrame(), [], []\n    \n    if 'Temperature' in df.columns:\n        df['Temperature'] = df['Temperature'].astype(str).str.replace(r'$assumed$', '', regex=True)\n        \n        def parse_temp(temp_str):\n            if '-' in temp_str:\n                try:\n                    low, high = map(float, temp_str.split('-'))\n                    return (low + high) / 2\n                except:\n                    return np.nan\n            else:\n                return pd.to_numeric(temp_str, errors='coerce')\n        \n        df['Temperature'] = df['Temperature'].apply(parse_temp)\n        nan_before = df['Temperature'].isna().sum()\n        \n        if df['Temperature'].notna().any() and nan_before < len(df):\n            try:\n                imputer = KNNImputer(n_neighbors=5)\n                df['Temperature'] = imputer.fit_transform(df[['Temperature']])\n            except ValueError as e:\n                print(f\"[DEBUG] Warning: KNNImputer failed for Temperature ({e}). Filling NaN with mean.\")\n                mean_temp = df['Temperature'].mean()\n                df['Temperature'] = df['Temperature'].fillna(mean_temp if not pd.isna(mean_temp) else 298.0)\n        else:\n            mean_temp = df['Temperature'].mean()\n            df['Temperature'] = df['Temperature'].fillna(mean_temp if not pd.isna(mean_temp) else 298.0)\n        \n        print(f\"[DEBUG] Температура обработана. NaN до (после парсинга): {nan_before}, NaN после: {df['Temperature'].isna().sum()}\")\n    else:\n        print(\"[DEBUG] ВНИМАНИЕ: Столбец 'Temperature' не найден. Используем T=298K.\")\n        df['Temperature'] = 298.0\n    \n    if 'Affinity_mut_parsed' in df.columns and 'Affinity_wt_parsed' in df.columns:\n        R = 1.987e-3\n        df['Kd_mut'] = pd.to_numeric(df['Affinity_mut_parsed'], errors='coerce')\n        df['Kd_wt'] = pd.to_numeric(df['Affinity_wt_parsed'], errors='coerce')\n        \n        mask = (df['Kd_mut'].notnull()) & (df['Kd_wt'].notnull()) & \\\n               (df['Kd_wt'] > 0) & (df['Kd_mut'] > 0) & \\\n               (df['Temperature'].notnull()) & (df['Temperature'] > 0)\n        \n        df['ddG'] = np.nan\n        df.loc[mask, 'ddG'] = R * df.loc[mask, 'Temperature'] * np.log(df.loc[mask, 'Kd_mut'] / df.loc[mask, 'Kd_wt'])\n        \n        print(f\"[DEBUG] Строк с валидными Kd/T для ddG: {mask.sum()}\")\n        original_rows = df.shape[0]\n        df = df.dropna(subset=['ddG'])\n        print(f\"[DEBUG] Размер после dropna(ddG): {df.shape} (удалено {original_rows - df.shape[0]})\")\n        \n        if df.empty: return pd.DataFrame(), [], []\n        \n        original_rows = df.shape[0]\n        df = df[df['ddG'] != 0]\n        print(f\"[DEBUG] Размер после удаления ddG==0: {df.shape} (удалено {original_rows - df.shape[0]})\")\n        \n        if df.empty: return pd.DataFrame(), [], []\n        \n        df['sign_ddG'] = (df['ddG'] < 0).astype(int)\n        print(f\"[DEBUG] Распределение sign_ddG (1=стабилизация, 0=дестабилизация):\\n{df['sign_ddG'].value_counts(normalize=True)}\")\n    else:\n        print(\"[DEBUG] ОШИБКА: Столбцы аффинности не найдены.\")\n        return pd.DataFrame(), [], []\n    \n    df['Position'] = df['Mutation(s)_cleaned'].astype(str).str.extract(r'(\\d+)', expand=False).astype(float)\n    \n    if 'iMutation_Location(s)' in df.columns:\n        df['Location'] = df['iMutation_Location(s)'].fillna('Unknown')\n        location_map = {'COR': 'COR', 'RIM': 'SUR', 'SUR': 'SUR', 'SUP': 'INT', 'INT': 'INT'}\n        df['Location_General'] = df['Location'].map(location_map).fillna('Other')\n        print(f\"[DEBUG] Location_General categories:\\n{df['Location_General'].value_counts()}\")\n    else:\n        print(\"[DEBUG] ВНИМАНИЕ: столбец 'iMutation_Location(s)' не найден.\")\n        df['Location'] = 'Unknown'\n        df['Location_General'] = 'Unknown'\n    \n    for prop in ['volume', 'hydrophobicity']:\n        prop_map = {aa: props[prop] for aa, props in aa_properties.items()}\n        wt_props = df['WT_AA'].map(prop_map)\n        mut_props = df['MUT_AA'].map(prop_map)\n        df[f'delta_{prop}'] = mut_props - wt_props\n        df[f'WT_{prop}'] = wt_props\n        df[f'MUT_{prop}'] = mut_props\n    \n    print(\"[DEBUG] Физ.-хим. признаки рассчитаны.\")\n    \n    categorical_features = ['WT_AA', 'MUT_AA', 'Location_General']\n    numeric_features = ['Position', 'delta_volume', 'delta_hydrophobicity',\n                       'WT_volume', 'WT_hydrophobicity', 'MUT_volume', 'MUT_hydrophobicity']\n    \n    final_feature_cols = categorical_features + numeric_features\n    target_col = 'sign_ddG'\n    \n    missing_features = [col for col in final_feature_cols if col not in df.columns]\n    if missing_features:\n        print(f\"[DEBUG] ОШИБКА: Отсутствуют колонки признаков: {missing_features}\")\n        return pd.DataFrame(), [], []\n    \n    if target_col not in df.columns:\n        print(f\"[DEBUG] ОШИБКА: Отсутствует целевая колонка '{target_col}'.\")\n        return pd.DataFrame(), [], []\n    \n    original_rows = df.shape[0]\n    df_clean = df[final_feature_cols + [target_col]].copy()\n    df_clean = df_clean.dropna(subset=final_feature_cols + [target_col])\n    print(f\"[DEBUG] Размер после dropna(признаки+цель): {df_clean.shape} (удалено {original_rows - df_clean.shape[0]})\")\n    \n    for col in numeric_features:\n        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n    \n    original_rows = df_clean.shape[0]\n    df_clean = df_clean.dropna(subset=numeric_features)\n    print(f\"[DEBUG] Размер после dropna(num_features post-convert): {df_clean.shape} (удалено {original_rows - df_clean.shape[0]})\")\n    \n    if df_clean.empty:\n        print(\"[DEBUG] ОШИБКА: DataFrame пуст перед возвратом.\")\n    else:\n        print(f\"--- [DEBUG] Данные успешно обработаны. Финальный размер: {df_clean.shape} ---\")\n    \n    return df_clean, categorical_features, numeric_features\n\n# ========= 4. Функции Оценки и Подбора Порога =========\ndef calculate_metrics(y_true, y_pred, y_proba):\n    \"\"\"Рассчитывает набор метрик для бинарной классификации.\"\"\"\n    if y_proba.ndim > 1 and y_proba.shape[1] == 2:\n        y_proba = y_proba[:, 1]\n    elif y_proba.ndim > 1 and y_proba.shape[1] > 2:\n        raise ValueError(\"y_proba должен содержать вероятности для бинарной классификации\")\n    \n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    unique_preds = np.unique(y_pred)\n    pos_label = 1\n    \n    f1 = f1_score(y_true, y_pred, pos_label=pos_label, zero_division=0)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    avg_prec = 0.0\n    if pos_label in y_true:\n        try:\n            avg_prec = average_precision_score(y_true, y_proba, pos_label=pos_label)\n        except ValueError as e:\n            print(f\"  Warning calculating PR AUC: {e}. Setting to 0.0\")\n            avg_prec = 0.0\n    else:\n        print(f\"  Warning: Positive label {pos_label} not found in y_true for PR AUC calculation.\")\n        avg_prec = np.nan\n    \n    eps = 1e-15\n    y_proba_clipped = np.clip(y_proba, eps, 1 - eps)\n    logloss = log_loss(y_true, y_proba_clipped)\n    \n    roc_auc = 0.0\n    if len(np.unique(y_true)) > 1:\n        try:\n            roc_auc = roc_auc_score(y_true, y_proba)\n        except ValueError as e:\n            print(f\"  Warning calculating ROC AUC: {e}. Setting to 0.0\")\n            roc_auc = 0.0\n    else:\n        print(\"  Warning: Only one class present in y_true. ROC AUC is not defined, setting to 0.0.\")\n        roc_auc = 0.0\n    \n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n        'kappa': cohen_kappa_score(y_true, y_pred),\n        'f1': f1,\n        'mcc': mcc,\n        'logloss': logloss,\n        'roc_auc': roc_auc,\n        'pr_auc': avg_prec\n    }\n    return metrics\n\ndef find_optimal_threshold(y_true, y_proba, metric_func=f1_score, pos_label=1):\n    \"\"\"Находит оптимальный порог бинаризации по заданной метрике.\"\"\"\n    thresholds = np.linspace(0.01, 0.99, 100)\n    best_score = -1\n    best_threshold = 0.5\n    \n    if y_proba.ndim > 1 and y_proba.shape[1] == 2:\n        y_proba = y_proba[:, 1]\n    \n    y_true = np.asarray(y_true)\n    \n    for threshold in thresholds:\n        y_pred_temp = (y_proba >= threshold).astype(int)\n        score = metric_func(y_true, y_pred_temp, pos_label=pos_label, zero_division=0)\n        if score > best_score:\n            best_score = score\n            best_threshold = threshold\n    return best_threshold\n\n# ========= 5. Загрузка и Подготовка Данных =========\nprint(f\"Версия scikit-learn: {sklearn.__version__}\")\nprint(f\"Загрузка данных из: {DATA_PATH}\")\ndf_processed, categorical_features, numeric_features = load_and_preprocess_data(DATA_PATH)\n\nif df_processed.empty:\n    raise ValueError(\"Не удалось загрузить или обработать данные.\")\n\nprint(f\"Фильтрация данных для локации: {LOCATION_TO_ANALYZE}\")\nif LOCATION_TO_ANALYZE == 'Other':\n    df_loc = df_processed[df_processed['Location_General'] != 'COR']\nelif LOCATION_TO_ANALYZE == 'COR':\n    df_loc = df_processed[df_processed['Location_General'] == 'COR']\nelse:\n    df_loc = df_processed.copy()\n\nif df_loc.empty:\n    raise ValueError(f\"Нет данных для выбранной локации: {LOCATION_TO_ANALYZE}\")\n\nprint(f\"Размер выборки после фильтрации: {df_loc.shape[0]}\")\n\nif 'sign_ddG' in df_loc.columns:\n    print(f\"Распределение классов:\\n{df_loc['sign_ddG'].value_counts(normalize=True)}\")\n    target_col = 'sign_ddG'\nelse:\n    raise ValueError(\"Целевая колонка 'sign_ddG' не найдена в отфильтрованных данных.\")\n\nX = df_loc[categorical_features + numeric_features]\ny = df_loc[target_col]\n\nprint(f\"NaN в X перед split: {X.isna().sum().sum()}\")\nprint(f\"NaN в y перед split: {y.isna().sum().sum()}\")\n\nif X.isna().sum().sum() > 0 or y.isna().sum().sum() > 0:\n    print(\"Warning: Обнаружены NaN перед разделением, хотя они должны были быть удалены. Повторная очистка...\")\n    df_loc = df_loc.dropna(subset=categorical_features + numeric_features + [target_col])\n    X = df_loc[categorical_features + numeric_features]\n    y = df_loc[target_col]\n    print(f\"Новый размер выборки после очистки: {df_loc.shape[0]}\")\n\nprint(f\"Разделение данных: test_size={TEST_SIZE}, random_state={RANDOM_STATE}\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n                                                    random_state=RANDOM_STATE, stratify=y)\nprint(f\"Размер обучающей выборки: {X_train.shape}, тестовой: {X_test.shape}\")\nprint(f\"Распределение классов в train:\\n{y_train.value_counts(normalize=True)}\")\nprint(f\"Распределение классов в test:\\n{y_test.value_counts(normalize=True)}\")\n\n# ========= 6. Препроцессор =========\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ],\n    remainder='passthrough',\n    verbose_feature_names_out=False\n)\n\ntry:\n    preprocessor.set_output(transform='pandas')\n    PREPROCESSOR_OUTPUT_PANDAS = True\n    print(\"Препроцессор будет выводить Pandas DataFrame.\")\nexcept AttributeError:\n    PREPROCESSOR_OUTPUT_PANDAS = False\n    print(\"Warning: preprocessor.set_output не доступен. Вывод будет NumPy array.\")\n\n# ========= 7. Лучшие Модели с Гиперпараметрической Оптимизацией =========\nrf_params = {\n    'n_estimators': IntDistribution(100, 500),\n    'max_depth': IntDistribution(3, 20),\n    'min_samples_split': IntDistribution(2, 11),\n    'class_weight': CategoricalDistribution(['balanced', None])\n}\n\ngb_params = {\n    'n_estimators': IntDistribution(100, 500),\n    'learning_rate': FloatDistribution(0.01, 0.3),\n    'max_depth': IntDistribution(3, 15),\n    'subsample': FloatDistribution(0.6, 1.0)\n}\n\nsvc_params = {\n    'C': FloatDistribution(0.1, 100),\n    'kernel': CategoricalDistribution(['rbf', 'poly']),\n    'gamma': CategoricalDistribution(['scale', 'auto'])\n}\n\nopt_rf = OptunaSearchCV(\n    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n    param_distributions=rf_params,\n    cv=StratifiedKFold(n_splits=CV_FOLDS),\n    n_trials=30,\n    scoring='f1',\n    n_jobs=-1\n)\n\nopt_gb = OptunaSearchCV(\n    estimator=GradientBoostingClassifier(random_state=RANDOM_STATE),\n    param_distributions=gb_params,\n    cv=StratifiedKFold(n_splits=CV_FOLDS),\n    n_trials=30,\n    scoring='f1',\n    n_jobs=-1\n)\n\n# ========= 8. Ансамблевая Модель =========\nstacking_model = StackingClassifier(\n    estimators=[\n        ('rf', opt_rf),\n        ('gb', opt_gb),\n        ('lr', LogisticRegression(max_iter=1000))\n    ],\n    final_estimator=LogisticRegression(),\n    cv=StratifiedKFold(n_splits=CV_FOLDS)\n)\n\n# ========= 9. Пайплайн с Ресэмплингом =========\npipeline = ImbPipeline([\n    ('preprocessor', preprocessor),\n    ('feature_selector', SelectKBest(score_func=f_classif, k=N_FEATURES_TO_SELECT)),\n    ('resampler', SMOTE(random_state=RANDOM_STATE)),\n    ('classifier', stacking_model)\n])\n\n# ========= 10. Обучение и Оценка =========\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\ny_proba = pipeline.predict_proba(X_test)[:, 1]\n\nmetrics = calculate_metrics(y_test, y_pred, y_proba)\n\n# ========= 11. Интерпретация =========\nexplainer = shap.Explainer(pipeline.named_steps['classifier'])\nshap_values = explainer.shap_values(pipeline.named_steps['preprocessor'].transform(X_test))\n\nplt.figure(figsize=(12, 6))\nshap.summary_plot(shap_values, pipeline.named_steps['preprocessor'].transform(X_test))\nplt.tight_layout()\nplt.savefig('shap_summary_2.png')\n\n# ========= 12. Сохранение Результатов =========\nresults = {\n    'best_params': {\n        'rf': opt_rf.best_params_,\n        'gb': opt_gb.best_params_\n    },\n    'metrics': metrics,\n    'feature_importance': dict(zip(\n        pipeline.named_steps['preprocessor'].get_feature_names_out(),\n        pipeline.named_steps['classifier'].feature_importances_\n    ))\n}\n\nwith open('enhanced_model_results_2.json', 'w') as f:\n    json.dump(results, f, indent=4)\n\nimport joblib\njoblib.dump(pipeline, 'enhanced_model_2.joblib')\n\nprint(\"Расширенная модель успешно обучена и сохранена!\")","block_group":"4e4a80e9d5d84a0e8e3473024a715dc5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=25296fc4-2e1a-40e5-990d-2e3a019f757c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"d3bcd82e0d50421d83de0a5fe09d92a7"}}